[
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Mathatistics",
    "section": "",
    "text": "Raju Rimal\n\n\nStatistician/ Data scientist\n\n\n\n\n\nPassionate about data analysis and visualisations with strong technical and interpersonal skills for working in a team. Searching career in the intersection of statistics, web and programming.\n\n\n\n\nraju.rimal@medisin.uio.no rimalraju@hotmail.com\n\n\n\nhttps://mathatistics.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDriving licence class B\n\n\n\nDownload PDF\n\n\n\n\n\n\nExperiencesEducationSkillsLanguage, Interests & Hobbies\n\n\n\nResearch\n\n\nPostdoctoral Fellow, University of Oslo\n\n\nFaculty of Medicine, Department of Biostatistics\n\n\nOslo, Norway\n\n\nNov 2020–Now\n\n\nAnalyzing the trend of melanoma incidence and survival by tumour thickness in Norway overall and within different prognostic factors such as sex, age, anatomic site, and melanoma subtypes.\n\n\n\n\n\nPh.D. Candidate, Norwegian University of Life Sciences\n\n\nFaculty of Chem., Biotech. and Food Science,\n\n\nÅs, Norway\n\n\nAug. 2015–Oct. 2019\n\n\nDeveloping a versatile tool for simulating multi-response data from a linear model.\n\n\n\n\n\nConsulting & Advising\n\n\nSenior Consultant, Norwegian University of Life Sciences\n\n\nCenral Adminstration,\n\n\nÅs, Norway\n\n\nNov. 2019–May. 2020\n\n\nMapping the research works of NMBU to the SDG goals of UN is the primary objective of this project. Creating web-application from scratch and analysing & visualizing the collected data has been an exciting experience.\n\n\n\n\n\nConsultant, Norwegian University of Life Sciences\n\n\nLearning Center,\n\n\nÅs, Norway\n\n\nNov. 2016–Mar. 2018\n\n\nProviding statistical advice and tips about data analysis and preparation to the bachelors, masters and other PhD students was both exciting and challenging job where I encountered problems from many disciplines such as animal sciences, plant sciences and econometrics.\n\n\n\n\n\n\n\n\nTeaching\n\n\nNorwegian University of Life Sciences\nÅs, Norway\n\nTheoretical Statistics II\nSpring 2020\nPlanning and perparation of the complete course,\nprepare exercises, assignments and examination.\n\n\n\nApplied Methods in Statistics\nAutumn 2018\nInvolved in lecturing the part of the course including\nclassification and descrimination.\n\n\n\nDesign of Experiment and Analysis of Variance\nAugust 2017\nInvolved in lecturing the part of the course including\nclassification and descrimination.\n\n\n\n\n\n\n\nTeaching Assistant\n\n\nNorwegian University of Life Sciences\nÅs, Norway\n\nApplied Methods in Statistics\nAutumn of 2016, 2017, and 2018\n\n\n\nRegression Analysis\nJanuary of 2014, 2016, and 2017\n\n\n\nDesign of Experiment and Analysis of Variance\nAugust of 2013, 2015, 2016, and 2017\n\n\nWorking closely with students and preparing exercises for the course \nhelped me not only to understand and solve students’ problems but also \ngave me ideas on how to construct lectures, exercises and other activities \nfor future courses. The experience has also helped me to clearly \nunderstand many basic statistical concepts.\n\n\n\n\n\n\nIT Support\n\n\nElection Commission Nepal\nAug 2011–July 2012\n\n\n\nPostal Service Department, Nepal\nMar 2005–Aug 2011\n\n\n\n\n\n\n\nDissertations and Publications\n\n\n\n\nThesis\n\n\nRimal, Raju. Exploration of Multi-Response Multivariate Methods PhD thesis. Norwegian University of Life Sciences, Ås, 2019.\n\nhttps://therimalaya.github.io/Thesis/Thesis.pdf\n\n\n\nRimal, Raju. Evaluation of models for predicting the average monthly Euro versus Norwegian krone exchange rate from financial and commodity information. MS thesis. Norwegian University of Life Sciences, Ås, 2014.\n\nhttps://doi.org/10.13140/rg.2.1.1145.4886\n\n\n\nPapers\n\n\nRimal, Raju, Almøy Trygve, and Sæbø Solve. A tool for simulating multi-response linear model data. Chemometrics and Intelligent Laboratory Systems 176 (2018): 1-10.\n\nhttps://doi.org/10.1016/j.chemolab.2018.02.009\n\n\n\nHelland, Inge S., Sæbø, solve, Almøy, Trygve and Rimal, Raju, 2018. Model and estimators for partial least squares regression. Journal of Chemometrics, 32(9), p.e3044.\n\nhttps://doi.org/10.1002/cem.3044\n\n\n\nRimal, R., Almøy, T., & Sæbø, S. (2019). Comparison of multi-response prediction methods. Chemometrics and Intelligent Laboratory Systems, 190, 10-21.\n\nhttps://doi.org/10.1016/j.chemolab.2019.05.004\n\n\n\nRimal, Raju, Almøy, Trygve, and Sæbø Solve. 2019. Comparison of multi-response estimation methods Chemometrics and Intelligent Laboratory Systems 205 (2020) 104093\n\nhttps://doi.org/10.1016/j.chemolab.2020.104093\n\n\n\n\n\nConference Participation\n\n\nTrends in melanoma tumour thickness in Norway, 1983–2019\n\n\nANCR Symposium, 2022\n\n\nTórshavn, Faroe Islands, Poster presentation\n\n\n\n\nLong-term Trend on Tumour Thickness of Melanoma in Norway\n\n\nNOFE Conference 2021\n\n\nBergen, Norway, Presentation\n\n\n\n\nLong-term trend in melanoma tumour thickness in Norway\n\n\nTenth World Congress of Melanoma in Conjunction with 17th EADO Congress\n\n\nVirtual (Online), Poster presentation\n\n\n\n\nSimrel-M: Simulation tool for multi-response linear model data\n\n\nThe 26th Nordic Conference in Mathematical Statistics (NordStat 2016)\n\n\nCopenhagen, Denmark, Poster presentation\n\n\n\n\nSimrel-M: A simulation tool and its application\n\n\nThe 19th national meeting of statistics (Norske Statistikermøtet)\n\n\nFredrikstad, Norway, Presentation\n\n\n\n\nA versatile tool for simulating linear model data\n\n\nUseR! 2018 Conference\n\n\nBrisbane, Australia, Presentation\n\n\n\n\nA comparison of multi-response prediction methods\n\n\nScandinavian Symposium on Chemometrics (SSC16) Conference\n\n\nOslo, Norway, Presentation\n\n\n\n\nMultimatrix Extension of Partial Least Square\n\n\nThe first annual conference of NORBIS\n\n\nRosendal Fjordhotel, Norway, Poster presentation\n\n\n\n\n\nAcademic Qualifications\n\n\nDoctor of Philosophy in Applied Statistics\n\n\nNorwegian University of Life Sciences\n\n\nÅs, Norway\n\n\n2015–2019\n\n\n\n\n\n\nMasters in Bioinformatics and Applied Statistics\n\n\nNorwegian University of Life Sciences\n\n\nÅs, Norway\n\n\n2012–2015\n\n\n\n\n\n\nMasters in Statistics\n\n\nTribhuwan University, Central Department of Statistics\n\n\nKathmandu, Nepal\n\n\n2007–2009\n\n\n\n\n\n\nBachelors in Physics and Statistics\n\n\nTribhuwan University, Tri-candra Campus\n\n\nKathmandu, Nepal\n\n\n2003–2007\n\n\n\n\n\n\n\n\nIT skills\n\n\n\nAdvanced user of R and comfortable with Matlab and Python in most situations.\n\n\n\n\n\n\n\n\n\n\nadvanced\n\nR, Python, LaTeX, Knitr, Rmarkdown, Web Technologies, Microsoft Office, Stata\n\n\n\ncomfortable\n\nGit, JavaScript, Minitab, Matlab, Docker, SQL, Bash, Linux, D3, Graphics Design\n\n\n\nelementary\n\nJulia, SPSS, SVG\n\n\n\n\n\n\n\n\n\nData cleaning and wrangling\nInteractive shiny application with R\nReproducible workflow\nVersion control and testing\nData visualization\nWeb development and deployment\n\n\n\n\n\n\n\n\n\n\nScientific skills\n\n\n\n\nData cleaning and wrangling\nInteractive shiny application with R\nReproducible workflow\nVersion control and testing\nData visualization\nWeb development and deployment\n\n\n\n\n\n\n\nInterpersonal skills\n\n\n\n\nData cleaning and wrangling\nInteractive shiny application with R\nReproducible workflow\nVersion control and testing\nData visualization\nWeb development and deployment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Reading\n      Writing\n      Speaking\n      Listening\n    \n  \n  \n    Nepali\nNative\nNative\nNative\nNative\n    English\nExcellent\nExcellent\nExcellent\nExcellent\n    Hindi\nVery Good\nGood\nGood\nExcellent\n    Norwegian\nNovice (A2)\nNovice (A2)\nNovice (A2)\nNovice (A2)\n  \n  \n  \n\n\n\n\n\nI like hiking, cycling, and photography. In my free time I also play lego and make crafts together with my son.\nI like programming and like to explore new programming language"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Raju Rimal",
    "section": "",
    "text": "Hi,\n\n\nI am a statistician and data scientist currently engaged in a project analyzing melanoma incidence and survival trend in Norway. My research interest lies in the intersection of statistics, data visualization, web and programming.\nI live in Drammen, a beautiful city in Norway, with my wife and a son. I spend my personal time mostly cycling, hiking and taking pictures. I also like listining music and sometimes binge watching movies."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Melanoma Incidence in Nordic Countries\n\n\n\n\nJune 20, 2023\n\n\nRaju Rimal\n\n\n17 min\n\n\n\n\n\n\n\n\nAge Adjusted Rates in Epidemiology\n\n\n\n\nJune 20, 2023\n\n\nTheRimalaya\n\n\n8 min\n\n\n\n\n\n\n\n\nSimulating data for ANOVA similar to existing dataset for analysis\n\n\n\n\nFebruary 4, 2023\n\n\nTheRimalaya\n\n\n13 min\n\n\n\n\n\n\n\n\nHow ANOVA analyze the variance\n\n\n\n\nMarch 29, 2021\n\n\nTheRimalaya\n\n\n5 min\n\n\n\n\n\n\n\n\nInterpretating Biplot\n\n\nA tool for exploring multivariate data\n\n\n\n\nMarch 18, 2017\n\n\nTheRimalaya\n\n\n5 min\n\n\n\n\n\n\n\n\nModel assessment and variable selection\n\n\nMaking simpler model for complex analysis\n\n\n\n\nMarch 5, 2017\n\n\nTheRimalaya\n\n\n7 min\n\n\n\n\n\n\n\n\nImporting and Exporting data in R\n\n\n\n\nMarch 1, 2017\n\n\nTheRimalaya\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html",
    "href": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html",
    "title": "Simulating data for ANOVA similar to existing dataset for analysis",
    "section": "",
    "text": "Simulating data is an important tool in both education and research, and it has been extremely helpful for testing, comparing, and understanding concepts in practical and applied settings.\nOften, we use Analysis of Variance (ANOVA) to analyze variances to find out if different cases result in similar outcomes and if the differences are significant. Some simple examples include:\nThese are common examples where, in some cases, data are collected by setting up an experiment, and in other cases, they are collected through sampling. This article explains how ANOVA analyzes the variance and in what situations are they significant through both simulated and real data.\nConsider the following model with \\(i=3\\) groups and \\(j=n\\) observations,\n\\[\ny_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}, \\; i = 1, 2, 3\n\\texttt{ and } j = 1, 2, \\ldots n\n\\]\nHere, \\(\\tau_i\\) is the effect corresponding to group \\(i\\) and \\(\\varepsilon_{ij} \\sim \\mathrm{N}(0, \\sigma^2)\\), the usual assumption of linear model. The simulation example below describes it in detail."
  },
  {
    "objectID": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#simulation-design",
    "href": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#simulation-design",
    "title": "Simulating data for ANOVA similar to existing dataset for analysis",
    "section": "Simulation Design",
    "text": "Simulation Design\n\n\nSimulation design\nsim_design <- tidytable(\n  Illiteracy = factor(\n    rep(c(1, 2, 3), 3),\n    labels = c(\"high\", \"medium\", \"low\")\n  ),\n  Crime = factor(\n    rep(c(1, 2, 3), each = 3),\n    labels = c(\"Murder\", \"Assault\", \"Rape\")\n  ),\n  mean = c(11, 8, 5, 214, 190, 114, 23, 21, 19),\n  sd = c(3, 4, 3, 79, 82, 55, 8, 10, 10)\n)\n\nsim_design\n\n\n# A tidytable: 9 × 4\n  Illiteracy Crime    mean    sd\n  <fct>      <fct>   <dbl> <dbl>\n1 high       Murder     11     3\n2 medium     Murder      8     4\n3 low        Murder      5     3\n4 high       Assault   214    79\n5 medium     Assault   190    82\n6 low        Assault   114    55\n7 high       Rape       23     8\n8 medium     Rape       21    10\n9 low        Rape       19    10\n\n\nSince these data cannot contain negative values so instead of using rnorm available in stats package, I will use truncnorm available in GitHub. There are other options as well which can be used such as: …\nIf not installed, install the package as remotes::install_github(\"olafmersmann/truncnorm\") or devtools::install_github(\"olafmersmann/truncnorm\").\n\nCode for Simulation, Analysis, and Plot\nLet’s simulate 50 observation Arrest Rate in each levels of Illiteracy, and Crime in simulation design.\n\nnsim <- 50\n\n\nSimulation CodeData from simulation\n\n\n\nsim_data <- sim_design %>% \n  group_by(Illiteracy, Crime) %>% \n  mutate(rate = map2(mean, sd, ~tidytable(\n    Rate = truncnorm::rtruncnorm(\n      n = nsim, a = 0, b = Inf, mean = .x, sd = .y\n    ) %>% round()\n  ))) %>% \n  unnest() %>% ungroup() %>% \n  nest(.by = c(Crime))\n\nHere, Arrest rates were generated from a normal distribution using truncnorm from truncnorm package to get only positive value and also mirroring the mean and standard deviation in USArrests. Using normal distributions ensures the synthetic data mimics the actual data’s variation and mean, making the simulations realistic.\n\n\n\n\nSimulated data by group\nsim_data\n\n\n# A tidytable: 3 × 2\n  Crime   data                 \n  <fct>   <list>               \n1 Murder  <tidytable [150 × 4]>\n2 Assault <tidytable [150 × 4]>\n3 Rape    <tidytable [150 × 4]>\n\n\n\nMurderAssaultRape\n\n\n\n\nSimulated data for Murder\nhead(sim_data[Crime == \"Murder\", data][[1]])\n\n\n# A tidytable: 6 × 4\n  Illiteracy  mean    sd  Rate\n  <fct>      <dbl> <dbl> <dbl>\n1 high          11     3     7\n2 high          11     3    11\n3 high          11     3     9\n4 high          11     3    12\n5 high          11     3    15\n6 high          11     3    13\n\n\n\n\n\n\nSimulated data for Assault\nhead(sim_data[Crime == \"Assault\", data][[1]])\n\n\n# A tidytable: 6 × 4\n  Illiteracy  mean    sd  Rate\n  <fct>      <dbl> <dbl> <dbl>\n1 high         214    79   178\n2 high         214    79   163\n3 high         214    79   174\n4 high         214    79   234\n5 high         214    79   181\n6 high         214    79   139\n\n\n\n\n\n\nSimulated data for Rape\nhead(sim_data[Crime == \"Rape\", data][[1]])\n\n\n# A tidytable: 6 × 4\n  Illiteracy  mean    sd  Rate\n  <fct>      <dbl> <dbl> <dbl>\n1 high          23     8    25\n2 high          23     8    22\n3 high          23     8    16\n4 high          23     8    15\n5 high          23     8    35\n6 high          23     8    23\n\n\n\n\n\n\n\n\nUsing the simulated data above, we now fit an Anova model with Illiteracy as a factor (group) variable that affects the Arrest Rate (response variable) separately for each Crime. I have also made a density plot for the Rate variable for both simulated data and plot it with normal curve with corresponding mean and standard deviation. Following are the codes for fitting the Anova model, and creating density plot and box plot. Also we will perform a Posthoc test using Tukey’s method to make a pairwise comparison of different Illiteracy levels.\n\n\n\n\n\n\nCode for Model Fit and Plotting\n\n\n\n\n\n\nModel and plot dataDensity plotBox plotPosthoc plot\n\n\n\nmdl_fit <- sim_data %>% \n  mutate(\n    Fit = map(data, ~lm(Rate ~ Illiteracy, data = .x)),\n    Summary = map(Fit, summary),\n    Anova = map(Fit, anova),\n    Tukey = map(Fit, aov) %>% map(TukeyHSD)\n  )\n\nmdl_est <- mdl_fit %>% \n  summarize(\n    across(Summary, map, broom::tidy), \n    .by = c(Crime)\n  ) %>% unnest()\n\nmdl_fit_df <- mdl_fit %>% \n  summarize(\n    across(Fit, map, broom::augment),\n    .by = c(Crime)\n  ) %>% unnest()\n\neff_df <- mdl_fit %>% \n  summarize(\n    across(Fit, map, function(.fit) {\n      effects::Effect(\"Illiteracy\", .fit) %>%\n        as_tidytable()\n    }),\n    .by = \"Crime\"\n  ) %>% unnest()\n\nWarning in check_dep_version(): ABI version mismatch: \nlme4 was built with Matrix ABI version 1\nCurrent Matrix ABI version is 2\nPlease re-install lme4 from source or restore original 'Matrix' package\n\ntky_df <- mdl_fit %>% \n  summarize(\n    across(Tukey, function(tky) {\n      map(tky, purrr::pluck, \"Illiteracy\") %>%\n        map(as_tidytable, .keep_rownames = \"terms\")\n    }),\n    .by = \"Crime\"\n  ) %>% unnest()\n\n\n\n\ndensity_plot <- sim_data %>% \n  unnest(data) %>% \n  ggplot(aes(Rate, color = Illiteracy)) +\n    facet_wrap(\n      facets = vars(Crime),\n      ncol = 3,\n      scales = \"free\",\n    ) +\n    geom_density(aes(linetype = \"Simulated\")) +\n    geom_density(\n      aes(linetype = \"Fitted\", x = .fitted),\n      data = mdl_fit_df\n    ) +\n    geom_rug(\n      data = eff_df,\n      aes(x = fit)\n    ) +\n    scale_linetype_manual(\n      breaks = c(\"Simulated\", \"Fitted\"),\n      values = c(\"solid\", \"dashed\")\n    ) +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"bottom\") +\n    labs(\n      x = \"Arrest Rate\",\n      y = \"Density\",\n      linetype = NULL\n    )\n\n\n\n\neffect_plot <- mdl_fit_df %>% \n  ggplot(aes(Rate, Illiteracy)) +\n    facet_grid(\n      cols = vars(Crime),\n      scales = \"free_x\"\n    ) +\n    geom_boxplot(\n      notch = TRUE, \n      color = \"grey\",\n      outlier.colour = \"grey\"\n    ) +\n    geom_point(\n      position = position_jitter(height = 0.25),\n      color = \"grey\",\n      size = rel(0.9)\n    ) +\n    geom_pointrange(\n      aes(\n        color = \"Estimated\",\n        xmin = lower,\n        xmax = upper,\n        x = fit\n      ),\n      data = eff_df\n    ) +\n    geom_point(\n      aes(color = \"True Mean\", x = mean),\n      data = sim_design\n    ) +\n    scale_color_brewer(\n      name = \"Mean\",\n      palette = \"Set1\"\n    ) +\n    theme(\n      legend.position = \"bottom\"\n    )\n\n\n\n\ntukey_plot <- tky_df %>% \n  ggplot(aes(diff, terms)) +\n    facet_grid(\n      cols = vars(Crime), \n      scales = \"free_x\"\n    ) +\n    geom_pointrange(\n      aes(xmin = lwr, xmax = upr, x = diff),\n      shape = 21,\n      fill = \"whitesmoke\"\n    ) +\n    geom_vline(\n      xintercept = 0,\n      linetype = \"dashed\",\n      color = \"royalblue\"\n    ) +\n    scale_color_brewer(\n      name = \"Mean\",\n      palette = \"Set1\"\n    ) +\n    labs(\n      y = \"Illiteracy\",\n      x = \"Effect difference\",\n      title = \"Pairwise comparison of levels of illitracy\"\n    ) +\n    expand_limits(x = 0)"
  },
  {
    "objectID": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#analysis",
    "href": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#analysis",
    "title": "Simulating data for ANOVA similar to existing dataset for analysis",
    "section": "Analysis",
    "text": "Analysis\n\nDistributionModel FitEffect PlotPost-hoc\n\n\n\n\nDensity of simulated data\ndensity_plot\n\n\n\n\n\nHere, the kernel density plots for arrest rates were shown alongside the normal density curve. This visual assessment checked the goodness of fit between simulated data and the expected normal distribution. The close match between kernel density and normal density validates that the data follows a normal distribution, confirming the simulation’s accuracy.\n\n\nA one-way ANOVA output below helps to find if there is any difference between arrest rate based on illiteracy level for each crime. Here we see that in all crimes high illiteracy level was considered as reference and compared to this both medium and low illiteracy levels have lower arrest rate. This suggest that the higher illiteracy rate corresponds to higher arrest rate. However, for crimes: assault and rape, the effect of medium illiteracy rate has high p-value and can not be considered to have significant effect on arrest rate.\n\nMurderAssaultRape\n\n\n\n\nANOVA output for crime: Murder\nmdl_fit[Crime == \"Murder\", Summary][[1]]\n\n\n\nCall:\nlm(formula = Rate ~ Illiteracy, data = .x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.44  -2.31   0.08   1.95   7.56 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       10.9200     0.4247  25.713  < 2e-16 ***\nIlliteracymedium  -2.4800     0.6006  -4.129 6.08e-05 ***\nIlliteracylow     -6.0000     0.6006  -9.990  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.003 on 147 degrees of freedom\nMultiple R-squared:  0.4068,    Adjusted R-squared:  0.3987 \nF-statistic:  50.4 on 2 and 147 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nANOVA output for crime: Assault\nmdl_fit[Crime == \"Assault\", Summary][[1]]\n\n\n\nCall:\nlm(formula = Rate ~ Illiteracy, data = .x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-158.32  -42.22   -3.11   38.74  221.68 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        208.32       9.64  21.609  < 2e-16 ***\nIlliteracymedium    -5.00      13.63  -0.367    0.714    \nIlliteracylow      -95.42      13.63  -6.999 8.51e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68.17 on 147 degrees of freedom\nMultiple R-squared:  0.2969,    Adjusted R-squared:  0.2873 \nF-statistic: 31.03 on 2 and 147 DF,  p-value: 5.709e-12\n\n\n\n\n\n\nANOVA output for crime: Rape\nmdl_fit[Crime == \"Rape\", Summary][[1]]\n\n\n\nCall:\nlm(formula = Rate ~ Illiteracy, data = .x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-20.10  -6.10  -0.11   5.56  20.90 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        22.440      1.219  18.403  < 2e-16 ***\nIlliteracymedium   -1.340      1.724  -0.777  0.43837    \nIlliteracylow      -5.320      1.724  -3.085  0.00243 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.622 on 147 degrees of freedom\nMultiple R-squared:  0.06547,   Adjusted R-squared:  0.05276 \nF-statistic:  5.15 on 2 and 147 DF,  p-value: 0.006894\n\n\n\n\n\n\n\n\n\nBoxplot with fitted and true mean\neffect_plot\n\n\n\n\n\nBoxplots displayed arrest rate distributions within each illiteracy group stratified by crime. Points were scattered for detailed visualization, along with fitted means and confidence intervals. Here for all crimes, higher illiteracy corresponds to higher arrest rate and is more visible in murder.\n\n\n\n\nPost-hoc plot comparing pairwise difference\ntukey_plot\n\n\n\n\n\nThe post-hoc plot has highlighted statistically significant differences between different levels of illiteracy. Here, all pairs of illiteracy levels differ significantly at 95% confidence level for Murder however there is not such significant difference between medium and high illiteracy level for assault and rape."
  },
  {
    "objectID": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#data-preparation-and-dataset",
    "href": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#data-preparation-and-dataset",
    "title": "Simulating data for ANOVA similar to existing dataset for analysis",
    "section": "Data preparation and Dataset",
    "text": "Data preparation and Dataset\nHere, I have used USArrests dataset excluding the crime UrbanPop and merged it with another dataset state.x77 using its Illiteracy variable for 50 states. The Illiteracy was than categorized using its quantiles into three categories low, medium, and high mimiking the simulation example above.\n\n\nMerging USArrests and state.x77\narrest <- as_tidytable(USArrests, .keep_rownames = \"States\") %>% \n  tidytable::left_join(\n    as_tidytable(\n      state.x77[, \"Illiteracy\", drop = FALSE],\n      .keep_rownames = \"States\"\n    ),\n    by = \"States\"\n  ) %>% \n  select(-UrbanPop) %>% \n  mutate(across(Murder:Illiteracy, as.numeric)) %>% \n  mutate(Illiteracy = cut.default(\n    Illiteracy,\n    breaks = quantile(Illiteracy, c(0, 1/3, 2/3, 1)),\n    labels = c(\"low\", \"medium\", \"high\"),\n    include.lowest = TRUE\n  )) %>% \n  mutate(Illiteracy = factor(\n    Illiteracy,\n    levels = c(\"high\", \"medium\", \"low\")\n  )) %>% \n  pivot_longer(\n    cols = Murder:Rape,\n    names_to = \"Crime\",\n    values_to = \"Rate\"\n  ) %>% nest(.by = c(Crime))\n\n\n\nData by CrimeMurderAssaultRape\n\n\n\n\nData by group\narrest\n\n\n# A tidytable: 3 × 2\n  Crime   data                \n  <chr>   <list>              \n1 Murder  <tidytable [50 × 3]>\n2 Assault <tidytable [50 × 3]>\n3 Rape    <tidytable [50 × 3]>\n\n\n\n\n\n\nData for crime: Murder\nhead(arrest[Crime == \"Murder\", data][[1]], 3)\n\n\n# A tidytable: 3 × 3\n  States  Illiteracy  Rate\n  <chr>   <fct>      <dbl>\n1 Alabama high        13.2\n2 Alaska  high        10  \n3 Arizona high         8.1\n\n\n\n\n\n\nData for crime: Assault\nhead(arrest[Crime == \"Assault\", data][[1]], 3)\n\n\n# A tidytable: 3 × 3\n  States  Illiteracy  Rate\n  <chr>   <fct>      <dbl>\n1 Alabama high         236\n2 Alaska  high         263\n3 Arizona high         294\n\n\n\n\n\n\nData for crime: Rape\nhead(arrest[Crime == \"Rape\", data][[1]], 3)\n\n\n# A tidytable: 3 × 3\n  States  Illiteracy  Rate\n  <chr>   <fct>      <dbl>\n1 Alabama high        21.2\n2 Alaska  high        44.5\n3 Arizona high        31"
  },
  {
    "objectID": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#analysis-1",
    "href": "blog/posts/2023-02-04-simulation-anova-and-analysis/index.html#analysis-1",
    "title": "Simulating data for ANOVA similar to existing dataset for analysis",
    "section": "Analysis",
    "text": "Analysis\nI am following a similar pattern as in the analysis of simulated data: distribution plot, fitting an ANOVA model, effect plot showing the fitted value with a boxplot, and a Post-hoc showing pairwise comparison of the effect of illiteracy levels on arrest rate.\n\nDistributionFitEffectsPost-hoc\n\n\n\n\nCode\nggplot(unnest(arrest), aes(Rate, color = Illiteracy)) +\n  geom_density(aes(linetype = \"Simulated\")) +\n  geom_density(\n    aes(linetype = \"Fitted\", x = .fitted),\n    data = mdl_fit_df\n  ) +\n  geom_rug(\n    data = eff_df,\n    aes(x = fit)\n  ) +\n  scale_linetype_manual(\n    breaks = c(\"Simulated\", \"Fitted\"),\n    values = c(\"solid\", \"dashed\")\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  labs(\n    x = \"Crime\",\n    y = \"Density\",\n    linetype = NULL\n  ) +\n  facet_wrap(\n    facets = vars(Crime),\n    scales = \"free\"\n  )\n\n\n\n\n\nKernel density alongside normal density curves for each crime shows and validates the normal distribution of the real data and help confirm the normality assumption for ANOVA, ensuring that the real data analysis aligns with the assumptions necessary for valid inference.\n\n\n\nMurderAssultRape\n\n\n\n\nANOVA output for crime: Murder\nmdl_fit[Crime == \"Murder\", Summary][[1]]\n\n\n\nCall:\nlm(formula = Rate ~ Illiteracy, data = .x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7067 -2.3000 -0.3059  1.7882  7.8933 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       11.4118     0.8084  14.116  < 2e-16 ***\nIlliteracymedium  -3.9051     1.1808  -3.307  0.00181 ** \nIlliteracylow     -6.8118     1.1273  -6.043 2.32e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.333 on 47 degrees of freedom\nMultiple R-squared:  0.4382,    Adjusted R-squared:  0.4143 \nF-statistic: 18.33 on 2 and 47 DF,  p-value: 1.302e-06\n\n\n\n\n\n\nANOVA output for crime: Assault\nmdl_fit[Crime == \"Assault\", Summary][[1]]\n\n\n\nCall:\nlm(formula = Rate ~ Illiteracy, data = .x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-168.000  -41.792   -4.083   47.958  145.333 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        214.00      17.53  12.208 3.51e-16 ***\nIlliteracymedium   -24.33      25.60  -0.950 0.346771    \nIlliteracylow      -99.83      24.44  -4.084 0.000171 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 72.28 on 47 degrees of freedom\nMultiple R-squared:  0.2786,    Adjusted R-squared:  0.2479 \nF-statistic: 9.074 on 2 and 47 DF,  p-value: 0.0004653\n\n\n\n\n\n\nANOVA output for crime: Rape\nmdl_fit[Crime == \"Rape\", Summary][[1]]\n\n\n\nCall:\nlm(formula = Rate ~ Illiteracy, data = .x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.133  -6.259  -2.357   3.766  26.939 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        23.353      2.275  10.263 1.38e-13 ***\nIlliteracymedium   -1.920      3.323  -0.578    0.566    \nIlliteracylow      -4.292      3.173  -1.353    0.183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.382 on 47 degrees of freedom\nMultiple R-squared:  0.03766,   Adjusted R-squared:  -0.003286 \nF-statistic: 0.9198 on 2 and 47 DF,  p-value: 0.4057\n\n\n\n\n\n\n\n\n\nCode\nggplot(unnest(arrest), aes(Rate, Illiteracy)) +\n  geom_boxplot(\n    notch = FALSE,\n    color = \"grey\",\n    outlier.colour = \"grey\"\n  ) +\n  geom_point(\n    position = position_jitter(height = 0.25),\n    color = \"grey\",\n  ) +\n  geom_pointrange(\n    aes(\n      xmin = lower,\n      xmax = upper,\n      x = fit\n    ),\n    color = \"firebrick\",\n    data = eff_df\n  ) +\n  scale_color_brewer(\n    name = \"Mean\",\n    palette = \"Set1\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  facet_wrap(facets = vars(Crime), scales = \"free_x\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(tky_df, aes(diff, terms)) +\n  geom_pointrange(\n    aes(xmin = lwr, xmax = upr, x = diff),\n    shape = 21,\n    fill = \"whitesmoke\"\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    color = \"royalblue\"\n  ) +\n  scale_color_brewer(\n    name = \"Mean\",\n    palette = \"Set1\"\n  ) +\n  labs(\n    y = \"Illiteracy\",\n    x = \"Effect difference\",\n    title = \"Pairwise comparison of levels of illiteracy\"\n  ) +\n  expand_limits(x = 0) +\n  facet_wrap(facets = vars(Crime), scales = \"free_x\")\n\n\n\n\n\n\n\n\nThe analysis using both simulated and real datasets demonstrates the effectiveness of ANOVA in uncovering patterns. Simulating data that closely mirrors the USArrests dataset provided a controlled environment for testing and understanding variable interactions.\nWhen applied to real data, the analysis confirmed significant differences in arrest rates across illiteracy groups, validating the method. By comparing these results, I highlighted how well-designed simulations can replicate real-world scenarios, offering valuable insights and preparing for real-world analyses.\nThis approach underscores the utility of combining simulated and real data, showcasing the robustness and reliability of the analytical methods used."
  },
  {
    "objectID": "blog/posts/2023-06-20-nordic-melanoma/index.html",
    "href": "blog/posts/2023-06-20-nordic-melanoma/index.html",
    "title": "Melanoma Incidence in Nordic Countries",
    "section": "",
    "text": "Cutaneous melanoma (CM) is the most aggressive and lethal form of skin cancer. Melanoma can be cured if caught and treated early but if left untreated, it may spread to other parts and can be fatal. In the recent years, melanoma has increased dramatically in fair skinned population worldwide including Nordic countries like Norway, Denmark, and Sweden. Norway is ranked fifth in incidence and third in mortality worldwide. This increase can be an effect of increased awareness in general public and health care provider.\nThis article explores melanoma incidence and mortality in nordic countries by sex and their trend over 40-years period from 1980–2020. Further, I try to step through the analysis process from data collection to create plots and tables."
  },
  {
    "objectID": "blog/posts/2023-06-20-nordic-melanoma/index.html#data-preparation",
    "href": "blog/posts/2023-06-20-nordic-melanoma/index.html#data-preparation",
    "title": "Melanoma Incidence in Nordic Countries",
    "section": "Data Preparation",
    "text": "Data Preparation\nData on melanoma were obtained from NORDCAN Engholm et al. (2010), Association of the Nordic Cancer Registries, IARC. Using NORDCAN 2.0 API1 crude and age-adjusted rates were downloaded as JSON and converted to tabular data for further analysis. R(R Core Team 2020) software was used for data gathering, cleanup, analysis, and plotting.\nThe API endpoint has four placeholder type, sex, country, and cancer following design was used to create individual endpoint. These individual url are used to download the JSON file as list in R.\n\nData APISample JSONData downloadData Preparation\n\n\n\n\nCode for preparing download URL\ndesign_map <- list(\n  sex = c(Male = 1, Female = 2),\n  type = c(Incidence = 0, Mortality = 1),\n  cancer = c(Melanoma = 290),\n  country = c(\n    Denmark = 208, Finland = 246, Iceland = 352,\n    Norway = 578, Sweden = 752\n  )\n)\nlabel_values <- function(data, label_map, var) {\n  label_vec <- label_map[[var]]\n  label <- `names<-`(names(label_vec), label_vec)\n  data[[var]] <- data[, label[as.character(get(var))]]\n  return(data)\n}\n\ndesign <- do.call(crossing, design_map) %>% \n  mutate(\n    url = glue::glue( \n      \"https://gco.iarc.fr/gateway_prod/api/nordcan/v2/92/data/population/{type}/{sex}/({country})/({cancer})/?ages_group=5_17&year_start=1980&year_end=2020&year_grouped=0\"\n    )\n  )\n\ndesign <- design %>% \n  label_values(design_map, \"sex\") %>% \n  label_values(design_map, \"type\") %>% \n  label_values(design_map, \"cancer\") %>% \n  label_values(design_map, \"country\")\n\n\n\nAPI URL:\n\nhttps://gco.iarc.fr/gateway_prod/api/nordcan/v2/92/data/population/{type}/{sex}/({country})/({cancer})/?ages_group=5_17&year_start=1980&year_end=2020&year_grouped=0\n\n\nReplacing the placeholders type (Incidence: 0, Mortality: 1), sex (Male: 1, Female: 2), country (Denmark: 208, Finland: 246, Iceland: 352, Norway: 578, and Sweden: 752), and cancer (Melanoma: 290) prepare the data API.\n\n\n\n\nCode\njson_data = await FileAttachment(\"Data/nordcan.json\").json()\njson_data[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode for data download\nif (!file.exists(\"Data/nordcan-json.Rds\")) {\n  design[, data := map(url, ~read_json(.x) %>% purrr::pluck(\"dataset\"))]\n  saveRDS(design, file = \"Data/nordcan-json.Rds\")\n} else {\n  design <- readRDS(\"Data/nordcan-json.Rds\")\n}\ndesign <- design %>% \n  tidytable::select(sex, type, country, data)\n\n\n\n\n\n\nRate data frame\nrate_df <- design[, map_df(data, function(dta) {\n  out <- data.table(\n    year = map_int(dta, ~get(\"year\", .x)),\n    asr_w = map_dbl(dta, ~get(\"asr\", .x)),\n    asr_e = map_dbl(dta, ~get(\"asr_e\", .x)),\n    asr_n = map_dbl(dta, ~get(\"asr_n\", .x)),\n    crude_rate = map_dbl(dta, ~get(\"crude_rate\", .x)),\n    count = map_dbl(dta, ~get(\"total\", .x)),\n    population = map_dbl(dta, ~get(\"total_pop\", .x)),\n    cum_risk = map(dta, ~get(\"cum_risk\", .x)) %>% \n      unlist()\n  )\n  if (\"cum_risk\" %in% names(out)) {\n    out <- out %>% \n      mutate(cum_risk = as.numeric(cum_risk))\n  }\n  return(out)\n}), by = .(sex, type, country)]\n\n\n\n\nClasses 'tidytable', 'data.table' and 'data.frame': 820 obs. of  10 variables:\n $ sex       : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ type      : chr  \"Incidence\" \"Incidence\" \"Incidence\" \"Incidence\" ...\n $ country   : chr  \"Denmark\" \"Denmark\" \"Denmark\" \"Denmark\" ...\n $ year      : int  1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 ...\n $ asr_w     : num  11.9 10.9 12.2 13.4 13.8 ...\n $ asr_e     : num  12.7 12.1 13.1 14.5 15.3 ...\n $ asr_n     : num  13.5 13.1 14.1 15.2 16 ...\n $ crude_rate: num  12.4 12 13.1 14.3 14.8 ...\n $ count     : num  196 191 210 230 239 234 269 283 291 306 ...\n $ population: num  1585436 1593815 1601416 1609960 1618038 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\n\n\n\nRate by age\nrate_by_age <- design[, map_df(data, function(dta) {\n  year <- map_int(dta, ~get(\"year\", .x))\n  count_df <- map(dta, ~get(\"ages\", .x)) %>% \n    map_dfr(as_tidytable) %>% \n    cbind(year = year) %>% \n    pivot_longer(\n      cols = -\"year\",\n      names_to = \"age_group\",\n      values_to = \"count\"\n    )\n  pop_df <- map(dta, ~get(\"populations\", .x)) %>% \n    map_dfr(as_tidytable) %>%\n    cbind(year = year) %>% \n    pivot_longer(\n      cols = -\"year\",\n      names_to = \"age_group\",\n      values_to = \"population\"\n    )\n  asr_df <- map(dta, ~get(\"age_specific_rate\", .x)) %>% \n    map_dfr(as_tidytable) %>% \n    as_tidytable() %>%\n    cbind(year = as.numeric(year)) %>% \n    pivot_longer(\n      cols = -\"year\",\n      names_to = \"age_group\",\n      values_to = \"asr\"\n    )\n  out <- purrr::reduce(\n    list(count_df, pop_df, asr_df),\n    inner_join,\n    by = c(\"year\", \"age_group\")\n  )\n  age_lbl <- paste(\n    seq(0, 85, 5),\n    seq(0, 85, 5) + 4,\n    sep = \"-\"\n  )\n  age_lbl[length(age_lbl)] <- \"85+\"\n  names(age_lbl) <- 1:18\n\n  out %>% \n    mutate(age_group = age_lbl[age_group]) %>% \n    mutate(asr = as.numeric(asr)) %>% \n    mutate(across(c(year, count, population), as.integer))\n\n}), by = .(sex, type, country)]\n\n\n\n\nClasses 'tidytable', 'data.table' and 'data.frame': 14760 obs. of  8 variables:\n $ sex       : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ type      : chr  \"Incidence\" \"Incidence\" \"Incidence\" \"Incidence\" ...\n $ country   : chr  \"Denmark\" \"Denmark\" \"Denmark\" \"Denmark\" ...\n $ year      : int  1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 ...\n $ age_group : chr  \"0-4\" \"0-4\" \"0-4\" \"0-4\" ...\n $ count     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ population: int  164317 156964 150170 145414 139557 135827 134375 136240 138423 142597 ...\n $ asr       : num  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \".internal.selfref\")=<externalptr>"
  },
  {
    "objectID": "blog/posts/2023-06-20-nordic-melanoma/index.html#analysis",
    "href": "blog/posts/2023-06-20-nordic-melanoma/index.html#analysis",
    "title": "Melanoma Incidence in Nordic Countries",
    "section": "Analysis",
    "text": "Analysis\nFor the following analysis, crude rates were used in visualization and modelling. Stratified by sex, country and type following plots presents the the crude_rate over the year of diagnosis. Additionally, using count and population, a poisson regression model (Equation 1) was fitted.\n\\[\n\\begin{align}\n\\log\\left(\\frac{\\lambda}{Y}\\right) &= \\beta_0 + \\beta_1 x + \\varepsilon \\\\\n\\text{equivalently, } \\log\\left(\\lambda\\right) &= \\beta_0 + \\beta_1 x + \\log(Y) + \\varepsilon\n\\end{align}\n\\tag{1}\\]\nwhere, \\(\\lambda\\) is the number of events (count), \\(Y\\) is the number of exposed (population) and \\(x\\) is the year of diagnosis.\nAdditionally, using segmented regression the change points in the trend was identified and the annual percentage change (APC) and average annual percentage change (AAPC) in crude rate were calculated. For each strata, following R-code for poisson regression model and segmented regression model were used.\n\n\nData within each strata\nnested_df <- rate_df %>% \n  nest(data = -c(sex, type, country))\nhead(nested_df)\n\n\n# A tidytable: 6 × 4\n  sex   type      country data                \n  <chr> <chr>     <chr>   <list>              \n1 Male  Incidence Denmark <tidytable [41 × 7]>\n2 Male  Incidence Finland <tidytable [41 × 7]>\n3 Male  Incidence Iceland <tidytable [41 × 7]>\n4 Male  Incidence Norway  <tidytable [41 × 7]>\n5 Male  Incidence Sweden  <tidytable [41 × 7]>\n6 Male  Mortality Denmark <tidytable [41 × 7]>\n\n\n\nModelling\n\n\n\n\n\n\nPoisson regression model\n\n\n\nmodel <- glm(\n  count ~ year + offset(log(population)),\n  data = rate_df,\n  family = poisson(link = \"log\")\n)\n\n\n\n\n\n\n\n\nSegmented regression model\n\n\n\nsgmt_model <- segmented(model, npsi = 2)\n\n\n\n\nPoisson and segmented fit\nfitted_df <- nested_df %>% \n  mutate(fit = map(data, function(.data) {\n    glm(\n      count ~ year + offset(log(population)),\n      family = poisson(link = \"log\"),\n      data = .data\n    )\n  })) %>% \n  mutate(sgmt_fit = map2(data, fit, function(.data, .fit) {\n    out <- segmented(.fit, seg.Z = ~year, data = .data, npsi = 2)\n    if (!(\"segmented\" %in% class(out))) {\n      out <- segmented(.fit, seg.Z = ~year, data = .data, npsi = 1)\n    }\n    return(out)\n  }))\n\nhead(fitted_df)\n\n\n# A tidytable: 6 × 6\n  sex   type      country data                 fit    sgmt_fit  \n  <chr> <chr>     <chr>   <list>               <list> <list>    \n1 Male  Incidence Denmark <tidytable [41 × 7]> <glm>  <segmentd>\n2 Male  Incidence Finland <tidytable [41 × 7]> <glm>  <segmentd>\n3 Male  Incidence Iceland <tidytable [41 × 7]> <glm>  <segmentd>\n4 Male  Incidence Norway  <tidytable [41 × 7]> <glm>  <segmentd>\n5 Male  Incidence Sweden  <tidytable [41 × 7]> <glm>  <segmentd>\n6 Male  Mortality Denmark <tidytable [41 × 7]> <glm>  <segmentd>\n\n\nFollowing plots highlighting Norway and Finland for comparison show a higher melanoma incidence and mortality rate in Norway compared to Finland. A plateau was observed in melanoma incidence in Norway in both male and female.\n\nCrude ratePoisson fitSegmented fitAll countries\n\n\n\n\nCrude rate plot\ncols <- RColorBrewer::brewer.pal(fitted_df[, n_distinct(country)], \"Set1\") \nnames(cols) <- fitted_df[, unique(country)]\n\nrate_df %>% \n  filter(country != \"Iceland\") %>% \n  ggplot(aes(year, crude_rate, group = country)) +\n  facet_grid(\n    cols = vars(sex),\n    rows = vars(type),\n    scales = \"free_y\"\n  ) +\n  geom_line(color = \"lightgrey\") +\n  geom_point(\n    fill = \"whitesmoke\", \n    shape = 21, \n    color = \"lightgrey\",\n    stroke = 1,\n    size = 1\n  ) +\n  geom_line(\n    data = ~subset(.x, country %in% c(\"Finland\", \"Norway\")),\n    aes(color = country)\n  ) +\n  ggthemes::theme_few(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    panel.grid = element_line(color = \"#f0f0f0\")\n  ) +\n  scale_color_manual(breaks = names(cols), values = cols) +\n  labs(\n    x = \"Year of diagnosis\",\n    y = \"Crude rate per 100,000 person-years\",\n    color = \"Country\"\n  )\n\n\n\n\n\n\n\n\n\nFitted values from the model\nfitted_df <- fitted_df %>% \n  mutate(\n    fit_df = map(\n      fit, \n      function(.fit) {\n        new_data <- crossing(year = 1980:2020, population = 1e5)\n        tidytable(\n          year = new_data[, year],\n          .fitted = predict(.fit, newdata = new_data, type = \"response\")\n        )\n      }),\n    sgmt_fit_df = map(\n      sgmt_fit, function(.fit) {\n        new_data <- crossing(year = 1980:2020, population = 1e5)\n        pred_df <- predict(\n          .fit, newdata = new_data,\n          type = \"link\", interval = \"confidence\"\n        ) %>% exp() %>% apply(1:2, prod, 1e5) %>% \n        as_tidytable() %>% \n        rename_with(~c(\".fitted\", \".lower\", \".upper\"))\n        bind_cols(year = new_data[, year], pred_df)\n      })\n  )\n\n\n\n\nCrude rate with poisson fit\nfit_df <- fitted_df %>%\n  unnest(fit_df) %>%\n  filter(country != \"Iceland\")\n\nplot_poisson <- function(rate_df, fit_df, countries = NULL) {\n  if (is.null(countries)) {\n    countries <- rate_df[, country]\n  }\n  rate_df %>%\n    filter(country != \"Iceland\") %>%\n    ggplot(aes(year, crude_rate, group = country)) +\n    facet_grid(\n      cols = vars(sex),\n      rows = vars(type),\n      scales = \"free_y\"\n    ) +\n    geom_line(color = \"lightgrey\") +\n    geom_point(\n      fill = \"whitesmoke\",\n      shape = 21,\n      color = \"lightgrey\",\n      stroke = 1,\n      size = 1\n    ) +\n    geom_line(\n      data = ~ subset(fit_df, country %in% countries),\n      aes(color = country, y = .fitted),\n    ) +\n    ggthemes::theme_few(base_size = 14) +\n    theme(\n      legend.position = \"bottom\",\n      legend.justification = \"left\",\n      panel.grid = element_line(color = \"#f0f0f0\")\n    ) +\n    scale_color_manual(breaks = names(cols), values = cols) +\n    labs(\n      x = \"Year of diagnosis\",\n      y = \"Crude rate per 100,000 person-years\",\n      color = \"Country\"\n    )\n}\nplot_poisson(rate_df, fit_df, c(\"Finland\", \"Norway\"))\n\n\n\n\n\n\n\n\n\nCrude rate with poisson fit\nplot_segmented <- function(rate_df, fit_df, countries = NULL, show_poisson = T) {\n  sgmt_fit_df <- fitted_df %>% \n    filter(country != \"Iceland\") %>% \n    unnest(sgmt_fit_df)\n\n  if (is.null(countries)) {\n    countries <- rate_df[, unique(country)]\n  } else {\n    sgmt_fit_df <- sgmt_fit_df %>% filter(country %in% countries)\n  }\n\n  plt <- rate_df %>% \n    filter(country != \"Iceland\") %>% \n    ggplot(aes(year, crude_rate, group = country)) +\n    facet_grid(\n      cols = vars(sex),\n      rows = vars(type),\n      scales = \"free_y\"\n    ) +\n    geom_line(color = \"lightgrey\") +\n    geom_point(\n      fill = \"whitesmoke\", \n      shape = 21, \n      color = \"lightgrey\",\n      stroke = 1,\n      size = 1\n    ) +\n    geom_line(\n      data = ~subset(sgmt_fit_df, country %in% countries),\n      aes(color = country, y = .fitted),\n      linetype = if (show_poisson) \"dashed\" else \"solid\"\n    ) +\n    ggthemes::theme_few(base_size = 14) +\n    theme(\n      legend.position = \"bottom\",\n      legend.justification = \"left\",\n      panel.grid = element_line(color = \"#f0f0f0\")\n    ) +\n    scale_color_manual(breaks = names(cols), values = cols) +\n    labs(\n      x = \"Year of diagnosis\",\n      y = \"Crude rate per 100,000 person-years\",\n      color = \"Country\"\n    )\n\n    if (show_poisson) {\n      plt <- plt +\n        geom_line(\n          data = ~subset(fit_df, country %in% countries),\n          aes(color = country, y = .fitted),\n          alpha = 0.5\n        )\n    }\n    return(plt)\n}\nplot_segmented(rate_df, fit_df, c(\"Norway\", \"Finland\"))\n\n\n\n\n\n\n\n\nPoisson fitSegmented fit\n\n\n\n\nCrude rate with poisson fit for all countries\nplot_poisson(rate_df, fit_df)\n\n\n\n\n\n\n\n\n\nCrude rate with poisson fit for all countries\nplot_segmented(rate_df, fit_df, show_poisson = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nHere, Norway leads with the highest rates of melanoma incidence and mortality, while Finland shines with the lowest rates across both sexes. Recently, Denmark has surged ahead of Norway and Sweden in terms of melanoma incidence.\nInterestingly, Norway had a plateau in melanoma cases for a while, but most Nordic countries saw a rise in melanoma cases after 2005. The silver lining here is that all countries have experienced a drop in melanoma mortality in recent years, thanks to better detection, treatments, and awareness.\n\n\nAnnual percentge change (APC)\n\nPlotTable\n\n\n\n\nCode\ntidy_fit <- fitted_df %>% \n  transmute(tidy_fit = map(fit, function(.fit) {\n    broom::tidy(.fit, exponentiate = TRUE, conf.int = TRUE) %>% \n      filter(term == \"year\", country != \"Iceland\")\n  }), .by = c(sex, type, country)) %>% \n  unnest()\n\ntidy_fit %>% \n  mutate(country = factor(country, c(\n    \"Finland\", \"Denmark\",\n    \"Sweden\", \"Norway\"\n  ))) %>% \n  ggplot(aes(x = estimate, y = country, color = sex)) +\n  facet_grid(cols = vars(type)) +\n  geom_pointrange(\n    aes(xmin = conf.low, xmax = conf.high),\n    shape = 21,\n    fill = \"whitesmoke\",\n    position = position_dodge(width = 0.5)\n  ) +\n  geom_vline(xintercept = 1, linetype = 2, color = \"grey\") +\n  scale_color_brewer(palette = \"Set1\") +\n  expand_limits(x = 1) +\n  ggthemes::theme_few(base_size = 16) +\n  theme(\n    panel.grid = element_line(color = \"#f0f0f0\"),\n    legend.position = \"bottom\",\n    legend.justification = \"left\"\n  ) +\n  labs(\n    x = \"Percentage change in count\",\n    y = NULL\n  )\n\n\n\n\n\n\n\n\n\nCalculating annual percentage change\naapc_df <- fitted_df %>% \n  transmute(\n    aapc = map(sgmt_fit, function(.fit) {\n      aapc(.fit) %>% \n        t() %>% \n        as_tidytable() %>% \n        rename_with(\n          ~c(\"estimate\", \"std_err\", \"lower\", \"upper\")\n        ) %>% \n        mutate(psi = \"1980--2020\") %>% \n        mutate(\n          label = glue::glue(\n            \"{estimate} ({lower}, {upper})\",\n            .transformer = \\(d, e) round(get(d, e), 2)\n          )\n        )\n    }),\n    apc = map(sgmt_fit, function(fit) {\n      if (!\"segmented\" %in% class(fit)) return(NULL)\n      out <- slope(fit, APC = TRUE)[[1]] %>% \n        as_tidytable(.keep_rownames = \"segment\")\n      psi_start <- unname(c(1980, fit$psi[, \"Est.\"], 2020))\n      psi_end <- lead(psi_start, 1)\n      psi_range <- glue::glue(\n        \"{psi_start}--{psi_end}\",\n        .transformer = \\(d, e) round(get(d, e))\n      )\n      psi_range <- setdiff(psi_range, last(psi_range))\n      out %>% \n        mutate(psi = psi_range) %>% \n        rename_with(\n          .fn = ~c(\"estimate\", \"lower\", \"upper\"), \n          .cols = 2:4\n        ) %>% \n        mutate(label = glue::glue(\n          \"{estimate} ({lower}, {upper})\",\n          .transformer = \\(d, e) round(get(d, e), 2)\n        )) %>% \n        mutate(label_period = glue::glue(\"{label}<br>{psi}\")) %>% \n        mutate(segment = gsub(\"slope\", \"\", segment))\n    }),\n    .by = c(sex, type, country)\n  )\n\napc <- aapc_df %>% unnest(\"apc\")\naapc <- aapc_df %>% unnest(\"aapc\")\n\n\n\n\nTable data for APC\napc %>% \n  filter(country %in% c(\"Norway\", \"Denmark\")) %>% \n  pivot_wider(\n    id_cols = c(country, segment),\n    names_from = c(type, sex),\n    values_from = \"label_period\"\n  ) %>% \n  gt::gt(\n    id = \"apc-table\",\n    rowname_col = \"segment\",\n    groupname_col = \"country\"\n  ) %>% \n  gt::tab_spanner_delim(\"_\") %>% \n  gt::fmt_markdown(everything()) %>% \n  gt::sub_missing(everything(), missing_text = \"-\") %>%\n  gt::tab_stubhead(\"Segment\") %>% \n  gt::tab_options(\n    table.width = \"100%\",\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )\n\n\n\n\n\n\n  \n    \n      Segment\n      \n        Incidence\n      \n      \n        Mortality\n      \n    \n    \n      Male\n      Female\n      Male\n      Female\n    \n  \n  \n    \n      Denmark\n    \n    1\n3.54 (3.23, 3.85)1980–2004\n2.73 (2.4, 3.06)1980–2002\n0.63 (0.1, 1.17)1980–2004\n0.51 (0.12, 0.91)1980–2011\n    2\n9.8 (7.32, 12.33)2004–2009\n6.92 (5.96, 7.88)2002–2011\n4.1 (0.93, 7.37)2004–2012\n12.09 (-12.23, 43.14)2011–2013\n    3\n3.1 (2.53, 3.68)2009–2020\n1.79 (1.18, 2.41)2011–2020\n-2.15 (-4.08, -0.19)2012–2020\n-4.08 (-7.35, -0.69)2013–2020\n    \n      Norway\n    \n    1\n5.49 (4.32, 6.67)1980–1991\n3.87 (2.67, 5.09)1980–1990\n17.12 (-0.32, 37.61)1980–1982\n1.02 (0.19, 1.85)1980–2000\n    2\n0.45 (-0.44, 1.35)1991–2002\n0.17 (-0.69, 1.04)1990–2000\n1.78 (1.39, 2.18)1982–2011\n2.64 (1.25, 4.05)2000–2013\n    3\n4.39 (4.08, 4.69)2002–2020\n3.67 (3.39, 3.95)2000–2020\n-2.05 (-3.87, -0.2)2011–2020\n-5.28 (-8.41, -2.05)2013–2020\n  \n  \n  \n\n\n\n\n\n\n\n\n\nComparing countries\n\nIncidenceMortality\n\n\n\n\nCode\nmdl_inc <- glm(\n  data = rate_df,\n  formula = count ~ year + sex + country,\n  offset = log(population),\n  family = poisson(link = \"log\"),\n  subset = type == \"Incidence\"\n)\n\nbroom::tidy(mdl_inc, conf.int = TRUE, exponentiate = TRUE) %>% \n  mutate(across(c(2:4, 6:7), round, 3))\n\n\n# A tidytable: 7 × 7\n  term           estimate std.error statistic   p.value conf.low conf.high\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)       0         0.383   -205.   0            0         0    \n2 year              1.04      0        185.   0            1.04      1.04 \n3 sexMale           1.01      0.004      1.90 5.68e-  2    1         1.02 \n4 countryFinland    0.649     0.007    -62.6  0            0.64      0.658\n5 countryIceland    0.494     0.028    -25.6  1.86e-144    0.468     0.521\n6 countryNorway     1.05      0.006      8.09 5.84e- 16    1.04      1.06 \n7 countrySweden     0.936     0.005    -12.1  1.41e- 33    0.926     0.946\n\n\n\n\n\n\nCode\nmdl_mor <- glm(\n  data = rate_df,\n  formula = count ~ year + sex + country,\n  offset = log(population),\n  family = poisson(link = \"log\"),\n  subset = type == \"Mortality\"\n)\n\nbroom::tidy(mdl_mor, conf.int = TRUE, exponentiate = TRUE) %>% \n  mutate(across(c(2:4, 6:7), round, 3))\n\n\n# A tidytable: 7 × 7\n  term           estimate std.error statistic   p.value conf.low conf.high\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)       0         0.844    -40.9  0            0         0    \n2 year              1.01      0         29.2  1.53e-187    1.01      1.01 \n3 sexMale           1.46      0.01      38.0  0            1.43      1.49 \n4 countryFinland    0.731     0.016    -19.2  1.67e- 82    0.708     0.755\n5 countryIceland    0.611     0.061     -8.02 1.05e- 15    0.54      0.688\n6 countryNorway     1.24      0.015     14.6  3.57e- 48    1.20      1.27 \n7 countrySweden     1.03      0.013      2.2  2.78e-  2    1.00      1.06"
  },
  {
    "objectID": "blog/posts/2023-06-20-nordic-melanoma/index.html#summary",
    "href": "blog/posts/2023-06-20-nordic-melanoma/index.html#summary",
    "title": "Melanoma Incidence in Nordic Countries",
    "section": "Summary",
    "text": "Summary\nIn summary, melanoma trends across the Nordic countries reveal some concerning patterns. Incidence and mortality have been increasing in all countries, with Norway showing the highest mortality rate and the most substantial rise in both incidence and mortality. Denmark saw a particularly rapid increase in melanoma cases between 2002 and 2011 in women and between 2004 and 2009 in men, surpassing Norway during those periods.\nHowever, there is a silver lining: recent years have shown a decrease in melanoma mortality rates across all the Nordic countries. This positive trend suggests that advancements in medical treatments, early detection efforts, and greater public awareness are starting to make a difference. While the rise in incidence remains a challenge, the declining mortality rates provide a hopeful perspective moving forward."
  },
  {
    "objectID": "blog/posts/2023-06-20-nordic-melanoma/index.html#references",
    "href": "blog/posts/2023-06-20-nordic-melanoma/index.html#references",
    "title": "Melanoma Incidence in Nordic Countries",
    "section": "References",
    "text": "References\n\n\nEngholm, Gerda, Jacques Ferlay, Niels Christensen, Freddie Bray, Marianne L. Gjerstorff, Åsa Klint, Jóanis E. Køtlum, Elínborg Ólafsdóttir, Eero Pukkala, and Hans H. Storm. 2010. “NORDCAN – a Nordic Tool for Cancer Information, Planning, Quality Control and Research.” Acta Oncologica 49 (5): 725–36. https://doi.org/ch4598.\n\n\nLarønningen, S., J. Ferlay, H. Beydogan, F. Bray, G. Engholm, M. Ervik, J. Gulbrandsen, et al. 2022. “NORDCAN: Cancer Incidence, Mortality, Prevalence and Survival in the Nordic Countries, Version 9.2 (23.06.2022).” 2022. https://nordcan.iarc.fr/.\n\n\nMuggeo, Vito M. R. 2008. “Segmented: An r Package to Fit Regression Models with Broken-Line Relationships.” R News 8 (1): 20–25. https://cran.r-project.org/doc/Rnews/.\n\n\nNorway, Cancer Registry of. 2022. “Cancer in Norway 2021: Cancer Incidence, Mortality, Survival and Prevalence in Norway.” 0806-3621. Cancer Registry of Norway. https://www.kreftregisteret.no/globalassets/cancer-in-norway/2021/cin_report.pdf.\n\n\nR Core Team. 2020. “R: A Language and Environment for Statistical Computing.” Manual. Vienna, Austria. https://www.R-project.org/.\n\n\nWelch, H. Gilbert, Benjamin L. Mazer, and Adewole S. Adamson. 2021. “The Rapid Rise in Cutaneous Melanoma Diagnoses.” New England Journal of Medicine 384 (1): 72–79. https://doi.org/gm6vs4.\n\n\nWhiteman, David C., Adele C. Green, and Catherine M. Olsen. 2016. “The Growing Burden of Invasive Melanoma: Projections of Incidence Rates and Numbers of New Cases in Six Susceptible Populations Through 2031.” The Journal of Investigative Dermatology 136 (6): 1161–71. https://doi.org/f8psv4."
  },
  {
    "objectID": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html",
    "href": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html",
    "title": "Model assessment and variable selection",
    "section": "",
    "text": "Adding new variables to a model can introduce noise, complicating analysis. Simpler models tend to be better because they’re easier to understand and contain less noise. Statistical methods can help us choose the best variables and improve our models.\nThis tutorial will show you how to compare models to find the best one. Using the mtcars dataset available in R, it will explore methods for variable selection that help build efficient models with fewer variables. Here are two popular techniques for selecting the best subset of variables:"
  },
  {
    "objectID": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#completefull-model",
    "href": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#completefull-model",
    "title": "Model assessment and variable selection",
    "section": "Complete/full model",
    "text": "Complete/full model\n\n\nFitting a complete model\nfull.model <- lm(mpg ~ ., data = mtcars)\nsmry <- summary(full.model)\nsmry\n\n\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869, Adjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n\n\nHere, we can see that the model has explained almost 86.9 percent of variation present in mpg, but non of the predictors are significant. This is a hint of having unnecessary variables that has increased model error. Using regsubsets function from leaps package, we can select a subset of predictors based on some criteria."
  },
  {
    "objectID": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#selecting-best-subset",
    "href": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#selecting-best-subset",
    "title": "Model assessment and variable selection",
    "section": "Selecting best subset",
    "text": "Selecting best subset\n\n\nSelecting best subset model\nlibrary(leaps)\nbest.subset <- regsubsets(\n  x      = mtcars[, -1], # predictor variables\n  y      = mtcars[, 1], # response variable (mpg)\n  nbest  = 1, # top 1 best model\n  nvmax  = ncol(mtcars) - 1, # max. number of variable (all)\n  method = \"exhaustive\" # search all possible subset\n)\nbs.smry <- summary(best.subset)\n\n\nWe can combine following summary output with a plot created from additional estimates to get some insight. These estimates are also found in the summary object. The output show which variables are included with a star(*).\n\n\nSummary of best subset\nbs.smry$outmat %>% \n  as_tidytable(.keep_rownames = \"model\") %>% \n  gt::gt(rowname_col = \"model\") %>% \n  gt::opt_vertical_padding(0.5) %>% \n  gt::opt_row_striping() %>% \n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    stub.font.weight = \"bold\"\n  )\n\nbs.est <- tidytable(\n  nvar   = 1:(best.subset$nvmax - 1),\n  adj.r2 = round(bs.smry$adjr2, 3),\n  cp     = round(bs.smry$cp, 3),\n  bic    = round(bs.smry$bic, 3)\n) %>% pivot_longer(\n  cols = c(adj.r2:bic),\n  names_to = \"estimates\",\n  values_to = \"value\"\n)\n\n\n\n\n\n\n  \n    \n      \n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    1  ( 1 )\n \n \n \n \n*\n \n \n \n \n \n    2  ( 1 )\n*\n \n \n \n*\n \n \n \n \n \n    3  ( 1 )\n \n \n \n \n*\n*\n \n*\n \n \n    4  ( 1 )\n \n \n*\n \n*\n*\n \n*\n \n \n    5  ( 1 )\n \n*\n*\n \n*\n*\n \n*\n \n \n    6  ( 1 )\n \n*\n*\n*\n*\n*\n \n*\n \n \n    7  ( 1 )\n \n*\n*\n*\n*\n*\n \n*\n*\n \n    8  ( 1 )\n \n*\n*\n*\n*\n*\n \n*\n*\n*\n    9  ( 1 )\n \n*\n*\n*\n*\n*\n*\n*\n*\n*\n    10  ( 1 )\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n  \n  \n  \n\n\n\n\nWe can make a plot to visualise the properties of these individual models and select a model with specific number of predictor that can give minimum BIC, or minimum CP or maximum adjusted rsquared.\n\n\nPlotting Adj-Rsq, BIC, and CP\nbs.est.select <- bs.est %>%\n  group_by(estimates) %>%\n  filter(\n    (value == max(value) & estimates == \"adj.r2\") |\n      (value == min(value) & estimates != \"adj.r2\")\n  )\nggplot(bs.est, aes(nvar, value, color = estimates)) +\n  geom_point(shape = 21, fill = \"lightgray\") +\n  geom_line() +\n  facet_wrap(~estimates, scale = \"free_y\") +\n  theme(legend.position = \"top\") +\n  labs(\n    x = \"Number of variables in the model\",\n    y = \"Value of Estimate\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 10, 2)) +\n  geom_point(\n    data = bs.est.select, fill = \"red\",\n    shape = 21\n  ) +\n  geom_text(\n    aes(label = paste0(\"nvar:\", nvar, \"\\n\", \"value:\", value)),\n    data = bs.est.select,\n    size = 3, hjust = 0, vjust = c(1, -1, -1),\n    color = \"black\", family = \"monospace\"\n  )\n\n\n\n\n\nAdj-Rsq, BIC, and Mallows’ CP for models with increasing number of predictors\n\n\n\n\nFrom these plots, we see that with 5 variables we will obtain maximum adjusted coefficient of determination (\\(R^2\\)). Similarly, both BIC and Mallow CP will be minimum for models with only 3 predictor variables. With the help of table above, we can identify these variables. From the table, row corresponding to 3 variables, we see that the three predictors are wt, qsec and am. To obtain maximum adjusted \\(R^2\\), disp and hp should be added to the previous 3 predictors.\nThis way, we can reduce a model to few variables optimising different assessment criteria. Let look at the fit of these reduced models:\n\n\nReduced Model\nmodel.3 <- lm(mpg ~ wt + qsec + am, data = mtcars)\nmodel.5 <- update(model.3, . ~ . + disp + hp)"
  },
  {
    "objectID": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#model-summaries",
    "href": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#model-summaries",
    "title": "Model assessment and variable selection",
    "section": "Model summaries",
    "text": "Model summaries\n\n3 Variable Model5 Variable Model\n\n\n\n\nModel summary of model with 3 variables\nsummary(model.3)\n\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4811 -1.5555 -0.7257  1.4110  4.6610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.6178     6.9596   1.382 0.177915    \nwt           -3.9165     0.7112  -5.507 6.95e-06 ***\nqsec          1.2259     0.2887   4.247 0.000216 ***\nam            2.9358     1.4109   2.081 0.046716 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.459 on 28 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.8336 \nF-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11\n\n\n\n\n\n\nModel summary of model with 5 variables\nsummary(model.5)\n\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am + disp + hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5399 -1.7398 -0.3196  1.1676  4.5534 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 14.36190    9.74079   1.474  0.15238   \nwt          -4.08433    1.19410  -3.420  0.00208 **\nqsec         1.00690    0.47543   2.118  0.04391 * \nam           3.47045    1.48578   2.336  0.02749 * \ndisp         0.01124    0.01060   1.060  0.29897   \nhp          -0.02117    0.01450  -1.460  0.15639   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.429 on 26 degrees of freedom\nMultiple R-squared:  0.8637,    Adjusted R-squared:  0.8375 \nF-statistic: 32.96 on 5 and 26 DF,  p-value: 1.844e-10\n\n\n\n\n\nFrom these output, it seems that although adjusted \\(R^2\\) has increased in later model, the additional variables are not significant. we can compare these two model with an ANOVA test which compares the residual variance between these two models. We can write the hypothesis as,\n\\(H_0:\\) Model 1 and Model 2 are same vs \\(H_1:\\) Model 1 and Model 2 are different\nwhere, Model 1 and Model 2 represents 3 variable and 5 variable model\n\n\nAnova comparing Model with 3 and 5 variables\nanova(model.3, model.5)\n\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ wt + qsec + am\nModel 2: mpg ~ wt + qsec + am + disp + hp\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     28 169.29                           \n2     26 153.44  2    15.848 1.3427 0.2786\n\n\nThe ANOVA result can not reject the hypothesis so claim that Model 1 and Model 2 are same. So, it is better to select the simpler model with 3 predictor variables."
  },
  {
    "objectID": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#glossary",
    "href": "blog/posts/2017-03-05-model-assessment-and-variable-selection-prodecure/index.html#glossary",
    "title": "Model assessment and variable selection",
    "section": "Glossary",
    "text": "Glossary\n\nR-squared (R²): A statistical measure representing the proportion of the variance for the dependent variable that’s explained by the independent variables in the model. R² values range from 0 to 1, with higher values indicating better model performance.\nAdjusted R-squared (R² adjusted): Adjusted R² modifies R² to account for the number of predictors in the model. It provides a more accurate measure when comparing models with a different number of predictors, as it penalizes the addition of non-informative predictors.\nAkaike Information Criterion (AIC): AIC is an estimator of the relative quality of statistical models for a given set of data. It balances the goodness of fit of the model with the number of predictors, penalizing more complex models to avoid overfitting. Lower AIC values indicate better models.\nBayesian Information Criterion (BIC): Similar to AIC, BIC also evaluates model quality but imposes a more substantial penalty for the number of predictors. BIC is used to discourage overfitting, and lower BIC values indicate a better model.\nMallow’s CP: Mallow’s CP criterion assesses the trade-off between model complexity and goodness of fit. Lower values of CP are desired, with CP values close to the number of predictors plus one indicating well-fitted models."
  },
  {
    "objectID": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html",
    "href": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html",
    "title": "Importing and Exporting data in R",
    "section": "",
    "text": "Importing and loading data is a crucial skill in data analysis. R offers various methods to handle different data formats efficiently. This article covers the essential techniques for importing data into R, using datasets from different packages, and explores open data sources and useful resources."
  },
  {
    "objectID": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#different-formats-of-data",
    "href": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#different-formats-of-data",
    "title": "Importing and Exporting data in R",
    "section": "Different Formats of Data",
    "text": "Different Formats of Data\nData comes in various formats, each suited for different scenarios:\n\nCSV (Comma Separated Values): Simple text files where values are separated by commas.\nExcel: Commonly used .xls and .xlsx files.\nSQL Databases: Structured data stored in tables within relational databases.\nJSON (JavaScript Object Notation): Lightweight data interchange format.\nHTML: Webpage data.\nSPSS, SAS, Stata: Formats used by specialized statistical software.\nRData and RDS: Native R formats for storing R objects."
  },
  {
    "objectID": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#data-available-in-different-packages",
    "href": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#data-available-in-different-packages",
    "title": "Importing and Exporting data in R",
    "section": "Data Available in Different Packages",
    "text": "Data Available in Different Packages\nR comes with a plethora of packages that include built-in datasets, perfect for learning and practice:\n\ndatasetsggplot2dplyrMASScarData\n\n\nIncludes classic datasets like Iris, mtcars, and airquality.\n\nlibrary(datasets)\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nContains datasets such as mpg for practicing data visualization.\n\nlibrary(ggplot2)\ndata(mpg)\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n\n\n\nProvides the starwars and storms datasets, useful for demonstrating data manipulation techniques.\n\nlibrary(dplyr)\ndata(starwars)\nstr(starwars)\n\ntibble [87 × 14] (S3: tbl_df/tbl/data.frame)\n $ name      : chr [1:87] \"Luke Skywalker\" \"C-3PO\" \"R2-D2\" \"Darth Vader\" ...\n $ height    : int [1:87] 172 167 96 202 150 178 165 97 183 182 ...\n $ mass      : num [1:87] 77 75 32 136 49 120 75 32 84 77 ...\n $ hair_color: chr [1:87] \"blond\" NA NA \"none\" ...\n $ skin_color: chr [1:87] \"fair\" \"gold\" \"white, blue\" \"white\" ...\n $ eye_color : chr [1:87] \"blue\" \"yellow\" \"red\" \"yellow\" ...\n $ birth_year: num [1:87] 19 112 33 41.9 19 52 47 NA 24 57 ...\n $ sex       : chr [1:87] \"male\" \"none\" \"none\" \"male\" ...\n $ gender    : chr [1:87] \"masculine\" \"masculine\" \"masculine\" \"masculine\" ...\n $ homeworld : chr [1:87] \"Tatooine\" \"Tatooine\" \"Naboo\" \"Tatooine\" ...\n $ species   : chr [1:87] \"Human\" \"Droid\" \"Droid\" \"Human\" ...\n $ films     :List of 87\n  ..$ : chr [1:5] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"Revenge of the Sith\" ...\n  ..$ : chr [1:6] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"The Phantom Menace\" ...\n  ..$ : chr [1:7] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"The Phantom Menace\" ...\n  ..$ : chr [1:4] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"Revenge of the Sith\"\n  ..$ : chr [1:5] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"Revenge of the Sith\" ...\n  ..$ : chr [1:3] \"A New Hope\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:3] \"A New Hope\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"A New Hope\"\n  ..$ : chr \"A New Hope\"\n  ..$ : chr [1:6] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"The Phantom Menace\" ...\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"A New Hope\" \"Revenge of the Sith\"\n  ..$ : chr [1:5] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"Revenge of the Sith\" ...\n  ..$ : chr [1:4] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\" \"The Force Awakens\"\n  ..$ : chr \"A New Hope\"\n  ..$ : chr [1:3] \"A New Hope\" \"Return of the Jedi\" \"The Phantom Menace\"\n  ..$ : chr [1:3] \"A New Hope\" \"The Empire Strikes Back\" \"Return of the Jedi\"\n  ..$ : chr \"A New Hope\"\n  ..$ : chr [1:5] \"The Empire Strikes Back\" \"Return of the Jedi\" \"The Phantom Menace\" \"Attack of the Clones\" ...\n  ..$ : chr [1:5] \"The Empire Strikes Back\" \"Return of the Jedi\" \"The Phantom Menace\" \"Attack of the Clones\" ...\n  ..$ : chr [1:3] \"The Empire Strikes Back\" \"Return of the Jedi\" \"Attack of the Clones\"\n  ..$ : chr \"The Empire Strikes Back\"\n  ..$ : chr \"The Empire Strikes Back\"\n  ..$ : chr [1:2] \"The Empire Strikes Back\" \"Return of the Jedi\"\n  ..$ : chr \"The Empire Strikes Back\"\n  ..$ : chr [1:2] \"Return of the Jedi\" \"The Force Awakens\"\n  ..$ : chr \"Return of the Jedi\"\n  ..$ : chr \"Return of the Jedi\"\n  ..$ : chr \"Return of the Jedi\"\n  ..$ : chr \"Return of the Jedi\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"The Phantom Menace\" \"Attack of the Clones\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr [1:2] \"The Phantom Menace\" \"Attack of the Clones\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr [1:2] \"The Phantom Menace\" \"Attack of the Clones\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr \"Return of the Jedi\"\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"The Phantom Menace\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"The Phantom Menace\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"The Phantom Menace\" \"Revenge of the Sith\"\n  ..$ : chr \"The Phantom Menace\"\n  ..$ : chr [1:3] \"The Phantom Menace\" \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"The Phantom Menace\" \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr [1:2] \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr [1:2] \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr [1:2] \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr \"Attack of the Clones\"\n  ..$ : chr [1:2] \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"Revenge of the Sith\"\n  ..$ : chr \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"A New Hope\" \"Revenge of the Sith\"\n  ..$ : chr [1:2] \"Attack of the Clones\" \"Revenge of the Sith\"\n  ..$ : chr \"Revenge of the Sith\"\n  ..$ : chr \"The Force Awakens\"\n  ..$ : chr \"The Force Awakens\"\n  ..$ : chr \"The Force Awakens\"\n  ..$ : chr \"The Force Awakens\"\n  ..$ : chr \"The Force Awakens\"\n $ vehicles  :List of 87\n  ..$ : chr [1:2] \"Snowspeeder\" \"Imperial Speeder Bike\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Imperial Speeder Bike\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Tribubble bongo\"\n  ..$ : chr [1:2] \"Zephyr-G swoop bike\" \"XJ-6 airspeeder\"\n  ..$ : chr(0) \n  ..$ : chr \"AT-ST\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Snowspeeder\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Tribubble bongo\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Sith speeder\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Flitknot speeder\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Koro-2 Exodrive airspeeder\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Tsmeu-6 personal wheel bike\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n $ starships :List of 87\n  ..$ : chr [1:2] \"X-wing\" \"Imperial shuttle\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"TIE Advanced x1\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"X-wing\"\n  ..$ : chr [1:5] \"Jedi starfighter\" \"Trade Federation cruiser\" \"Naboo star skiff\" \"Jedi Interceptor\" ...\n  ..$ : chr [1:3] \"Naboo fighter\" \"Trade Federation cruiser\" \"Jedi Interceptor\"\n  ..$ : chr(0) \n  ..$ : chr [1:2] \"Millennium Falcon\" \"Imperial shuttle\"\n  ..$ : chr [1:2] \"Millennium Falcon\" \"Imperial shuttle\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"X-wing\"\n  ..$ : chr \"X-wing\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Slave 1\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Millennium Falcon\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"A-wing\"\n  ..$ : chr(0) \n  ..$ : chr \"Millennium Falcon\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr [1:3] \"Naboo fighter\" \"H-type Nubian yacht\" \"Naboo star skiff\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Naboo Royal Starship\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Scimitar\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Jedi starfighter\"\n  ..$ : chr(0) \n  ..$ : chr \"Naboo fighter\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"Belbullab-22 starfighter\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr(0) \n  ..$ : chr \"X-wing\"\n  ..$ : chr(0) \n  ..$ : chr(0) \n\n\n\n\nOffers datasets for applied statistics, including the Boston housing data.\n\nlibrary(MASS)\ndata(Boston)\nstr(Boston)\n\n'data.frame':   506 obs. of  14 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\n\n\n\nContains datasets ideal for regression, ANOVA, and generalized linear models.\n\nlibrary(carData)\ndata(MplsStops)\nstr(MplsStops)\n\n'data.frame':   51920 obs. of  14 variables:\n $ idNum         : Factor w/ 61212 levels \"16-395258\",\"16-395296\",..: 6823 6824 6825 6826 6827 6828 6829 6830 6831 6832 ...\n $ date          : POSIXct, format: \"2017-01-01 00:00:42\" \"2017-01-01 00:03:07\" ...\n $ problem       : Factor w/ 2 levels \"suspicious\",\"traffic\": 1 1 2 1 2 2 1 2 2 2 ...\n $ MDC           : Factor w/ 2 levels \"MDC\",\"other\": 1 1 1 1 1 1 1 1 1 1 ...\n $ citationIssued: Factor w/ 2 levels \"NO\",\"YES\": NA NA NA NA NA NA NA NA NA NA ...\n $ personSearch  : Factor w/ 2 levels \"NO\",\"YES\": 1 1 1 1 1 1 1 1 1 1 ...\n $ vehicleSearch : Factor w/ 2 levels \"NO\",\"YES\": 1 1 1 1 1 1 1 1 1 1 ...\n $ preRace       : Factor w/ 8 levels \"Black\",\"White\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ race          : Factor w/ 8 levels \"Black\",\"White\",..: 3 3 2 4 2 4 1 7 2 1 ...\n $ gender        : Factor w/ 3 levels \"Female\",\"Male\",..: 3 2 1 2 1 2 2 1 2 2 ...\n $ lat           : num  45 45 44.9 44.9 45 ...\n $ long          : num  -93.2 -93.3 -93.3 -93.3 -93.3 ...\n $ policePrecinct: int  1 1 5 5 1 1 1 2 2 4 ...\n $ neighborhood  : Factor w/ 87 levels \"Armatage\",\"Audubon Park\",..: 11 20 84 84 20 20 20 51 59 28 ..."
  },
  {
    "objectID": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#importing-different-data-formats-into-r",
    "href": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#importing-different-data-formats-into-r",
    "title": "Importing and Exporting data in R",
    "section": "Importing Different Data Formats into R",
    "text": "Importing Different Data Formats into R\nBefore diving into examples, let’s introduce some useful packages for data import:\n\nhaven: For SPSS, SAS, and Stata files.\ndata.table: Efficient data manipulation and import.\nreadxl: For reading Excel files.\njsonlite: For importing JSON files.\n\nHere is how you can import different data formats into R:\n\nCSVExcelSQLJSON\n\n\n\ndata <- read.csv(\"data/airtravel.csv\")\nhead(data)\n\n  Month X1958 X1959 X1960\n1   JAN   340   360   417\n2   FEB   318   342   391\n3   MAR   362   406   419\n4   APR   348   396   461\n5   MAY   363   420   472\n6   JUN   435   472   535\n\n\n\n\n\nlibrary(readxl)\ndata <- read_excel(\"data/Melanoma.xlsx\")\nhead(data)\n\n# A tibble: 6 × 7\n   time status   sex   age  year thickness ulcer\n  <dbl>  <dbl> <dbl> <dbl> <dbl>     <dbl> <dbl>\n1    10      3     1    76  1972      6.76     1\n2    30      3     1    56  1968      0.65     0\n3    35      2     1    41  1977      1.34     0\n4    99      3     0    71  1968      2.9      0\n5   185      1     1    52  1965     12.1      1\n6   204      1     1    28  1971      4.84     1\n\n\n\n\n\nlibrary(DBI)\ncon <- dbConnect(RSQLite::SQLite(), \"data/medal.db\")\ndata <- dbGetQuery(con, \"SELECT * FROM Olympic2024\")\nhead(data)\n\n               Team              Olympic No Gold Silver Bronze Total\n1 Afghanistan (AFG)       Combined total 16    0      0      2     2\n2 Afghanistan (AFG) Summer Olympic Games 16    0      0      2     2\n3 Afghanistan (AFG) Winter Olympic Games  0    0      0      0     0\n4     Albania (ALB)       Combined total 15    0      0      2     2\n5     Albania (ALB) Summer Olympic Games 10    0      0      2     2\n6     Albania (ALB) Winter Olympic Games  5    0      0      0     0\n\n\n\n\n\nlibrary(jsonlite)\ndata <- fromJSON(\"data/sdg-goals.json\")\nstr(data, list.len = 2)\n\n'data.frame':   17 obs. of  6 variables:\n $ goal     :List of 17\n  ..$ : int 1\n  ..$ : int 2\n  .. [list output truncated]\n $ title    :List of 17\n  ..$ : chr \"End poverty in all its forms everywhere\"\n  ..$ : chr \"End hunger, achieve food security and improved nutrition and promote sustainable agriculture\"\n  .. [list output truncated]\n  [list output truncated]\n\n\n\n\n\n\nHTMLRData and RDSSPSSStataSAS\n\n\n\nlibrary(rvest)\n# url <- \"https://en.wikipedia.org/wiki/Booker_Prize\"\nurl <- \"data/Booker-Prize.html\"\ndata <- read_html(url) %>%\n    html_node(\".wikitable\") %>%\n    html_table()\nhead(data)\n\n# A tibble: 6 × 5\n   Year Author              Title                   `Genre(s)`       hideCountry\n  <int> <chr>               <chr>                   <chr>            <chr>      \n1  1969 P. H. Newby[62]     Something to Answer For Literary fiction UK         \n2  1970 Bernice Rubens[63]  The Elected Member      Literary fiction UK         \n3  1971 V. S. Naipaul[64]   In a Free State         Literary fiction UK TTO     \n4  1972 John Berger[65]     G.                      Experimental li… UK         \n5  1973 J. G. Farrell[66]   The Siege of Krishnapur Literary fiction UK IRL     \n6  1974 Nadine Gordimer[67] The Conservationist     Literary fiction ZAF        \n\n\n\n\nWe can load RData file with load function and Rds file using readRDS function. RData file can contain multiple R objects and when loaded we can find the objects saved in RData file in R environment.\n\nload(\"data/cancer.rds\")\nhead(cancer)\n\n  inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n1    3  306      2  74   1       1       90       100     1175      NA\n2    3  455      2  68   1       0       90        90     1225      15\n3    3 1010      1  56   1       0       90        90       NA      15\n4    5  210      2  57   1       1       90        60     1150      11\n5    1  883      2  60   1       0      100        90       NA       0\n6   12 1022      1  74   1       1       50        80      513       0\n\npopmort <- readRDS(\"data/popmort.rds\")\nstr(popmort)\n\n 'ratetable' num [1:110, 1:2, 1:81] 1.47e-04 1.52e-05 7.92e-06 5.51e-06 4.44e-06 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ age : chr [1:110] \"0\" \"1\" \"2\" \"3\" ...\n  ..$ sex : chr [1:2] \"male\" \"female\"\n  ..$ year: chr [1:81] \"1940\" \"1941\" \"1942\" \"1943\" ...\n - attr(*, \"type\")= num [1:3] 2 1 4\n - attr(*, \"cutpoints\")=List of 3\n  ..$ : num [1:110] 0 365 730 1096 1461 ...\n  ..$ : NULL\n  ..$ : Date[1:81], format: \"1940-01-01\" \"1941-01-01\" ...\n - attr(*, \"summary\")=function (R)  \n  ..- attr(*, \"srcref\")= 'srcref' int [1:8] 7 13 15 3 13 3 7 15\n  .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x5d4b1a8a8440> \n\n\n\n\n\nlibrary(haven)\nspss_file <- read_sav(\"data/melanoma.sav\")\nhead(spss_file)\n\n# A tibble: 6 × 14\n  sex          age stage     mmdx  yydx surv_mm surv_yy status  subsite year8594\n  <dbl+lbl>  <dbl> <dbl+lb> <dbl> <dbl>   <dbl>   <dbl> <dbl+l> <dbl+l> <dbl+lb>\n1 2 [Female]    81 1 [Loca…     2  1981    26.5     2.5 2 [Dea… 1 [Hea… 0 [Diag…\n2 2 [Female]    75 1 [Loca…     9  1975    55.5     4.5 2 [Dea… 1 [Hea… 0 [Diag…\n3 2 [Female]    78 1 [Loca…     2  1978   178.     14.5 2 [Dea… 3 [Lim… 0 [Diag…\n4 2 [Female]    75 0 [Unkn…     8  1975    29.5     2.5 1 [Dea… 4 [Mul… 0 [Diag…\n5 2 [Female]    81 0 [Unkn…     7  1981    57.5     4.5 2 [Dea… 1 [Hea… 0 [Diag…\n6 2 [Female]    75 1 [Loca…     9  1975    19.5     1.5 1 [Dea… 2 [Tru… 0 [Diag…\n# ℹ 4 more variables: dx <date>, exit <date>, agegrp <dbl+lbl>, id <dbl>\n\n\n\n\n\nstata_file <- read_dta(\"data/colon.dta\")\nhead(stata_file)\n\n# A tibble: 6 × 14\n  sex          age stage     mmdx  yydx surv_mm surv_yy status  subsite year8594\n  <dbl+lbl>  <dbl> <dbl+lb> <dbl> <dbl>   <dbl>   <dbl> <dbl+l> <dbl+l> <dbl+lb>\n1 2 [Female]    77 3 [Dist…     3  1977    16.5     1.5 1 [Dea… 2 [Tra… 0 [Diag…\n2 2 [Female]    78 1 [Loca…     7  1978    82.5     6.5 2 [Dea… 1 [Coe… 0 [Diag…\n3 1 [Male]      78 3 [Dist…    10  1978     1.5     0.5 1 [Dea… 3 [Des… 0 [Diag…\n4 1 [Male]      76 3 [Dist…    10  1976     1.5     0.5 1 [Dea… 3 [Des… 0 [Diag…\n5 1 [Male]      80 1 [Loca…    12  1980     8.5     0.5 1 [Dea… 3 [Des… 0 [Diag…\n6 2 [Female]    75 1 [Loca…    11  1975    23.5     1.5 1 [Dea… 1 [Coe… 0 [Diag…\n# ℹ 4 more variables: agegrp <dbl+lbl>, dx <date>, exit <date>, id <dbl>\n\n\n\n\n\nsas_file <- read_xpt(\"data/mpg.xpt\")\nhead(sas_file)\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <dbl> <dbl> <chr>      <chr> <dbl> <dbl> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…"
  },
  {
    "objectID": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#exporting-data-to-different-formats-in-r",
    "href": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#exporting-data-to-different-formats-in-r",
    "title": "Importing and Exporting data in R",
    "section": "Exporting Data to Different Formats in R",
    "text": "Exporting Data to Different Formats in R\nR also allows you to export data to various formats:\n\nCSVExcelSQLJSONHTML\n\n\nwrite.csv(data, \"data.csv\")\n\n\nlibrary(writexl)\nwrite_xlsx(data, \"data.xlsx\")\n\n\nlibrary(DBI)\ncon <- dbConnect(RSQLite::SQLite(), \"example.db\")\ndbWriteTable(con, \"tablename\", data)\n\n\nlibrary(jsonlite)\nwrite_json(data, \"data.json\")\n\n\nlibrary(xml2)\nwrite_html(as_xml_document(as.character(data)), \"data.html\")\n\n\n\n\nRData and RDSSPSSStataSAS\n\n\nsave(data, file = \"data.RData\")\nsaveRDS(data, \"data.rds\")\n\n\nlibrary(haven)\nwrite_sav(data, \"data.sav\") # SPSS file\n\n\nlibrary(haven)\nwrite_dta(data, \"data.dta\") # Stata file\n\n\nlibrary(haven)\nwrite_xpt(data, \"data.xpt\") # SAS file"
  },
  {
    "objectID": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#available-open-data",
    "href": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#available-open-data",
    "title": "Importing and Exporting data in R",
    "section": "Available Open Data",
    "text": "Available Open Data\nThere are numerous open data sources available for free use:\n\nKaggle: A platform for data science competitions with a vast array of datasets.\nUCI Machine Learning Repository: A valuable resource for datasets widely used in machine learning.\nGovernment Portals: Websites such as data.gov and data.gov.uk provide a range of datasets.\nWorld Bank Open Data: Access to global development data."
  },
  {
    "objectID": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#useful-online-resources",
    "href": "blog/posts/2017-03-01-importing-and-exporting-data-in-R/index.html#useful-online-resources",
    "title": "Importing and Exporting data in R",
    "section": "Useful Online Resources",
    "text": "Useful Online Resources\nHere are some essential resources to deepen your understanding:\n\nDataset in R: Datasets from different package with CSV link and documentation\nR for Data Science: A comprehensive guide covering data import techniques.\nRDocumentation: Detailed documentation on R packages and functions.\nQuick-R: A quick reference for reading and writing data.\nTidyverse Blog: Articles and tutorials on data science using Tidyverse packages.\nDataCamp: Online courses on various topics, including data import in R."
  },
  {
    "objectID": "blog/posts/2017-03-18-interpreting-biplot/index.html",
    "href": "blog/posts/2017-03-18-interpreting-biplot/index.html",
    "title": "Interpretating Biplot",
    "section": "",
    "text": "A biplot is a powerful graphical tool that represents data in two dimensions, where both the observations and variables are represented. Biplots are particularly useful for multivariate data, allowing users to examine relationships between variables and identify patterns.\nConsider a data matrix \\(\\mathbf{X}_{n\\times p}\\) with \\(p\\) variables and \\(n\\) observations. To explore the data further with biplot, principal component analysis (PCA) is used here.\nPrincipal component analysis (PCA) compresses the variance of the data matrix to create a new set of orthogonal (linearly independent) variables \\(\\mathbf{Z} = \\begin{pmatrix}\\mathbf{z}_1 & \\mathbf{z}_2 & \\ldots & \\mathbf{z}_q\\end{pmatrix}\\), where \\(q = \\min(n, p)\\), often termed as principal components (PC) or scores. In PCA, the most variation is captured by the first component and rest in the subsequent components in decreasing order. In other words, the first principal component (\\(\\mathbf{z}_1\\)) captures the highest variation and second principal component (\\(\\mathbf{z}_2\\)) captures the maximum of remaing variation and so on.\nWe can use eigenvalue decomposition or singular value decompostion for this purpose.\nThese principle components are created using linear combination of the original variables. For example, \\(\\mathbf{z}_1 = w_{11}\\mathbf{x}_1 + \\ldots + w_{1p}\\mathbf{x}_p\\) and similarly for \\(\\mathbf{z}_2, \\ldots, \\mathbf{z}_q\\). Here we can estimate weights \\(w_{ij}\\), where \\(i = 1 \\ldots q\\) and \\(j = 1 \\ldots p\\) using eigenvalue decomposition or singular value (SVD). These weights are also refered as loading or the matrix of these weights as rotation matrix.\nBiplot plots the scores from two principal components together with loadings (weight) for each variables in the same plot. The following example uses USArrests data from datasets package in R. The dataset contains the number of arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973.\nTwo functions prcomp and princomp in R performs the principal component analysis. The function prcomp uses SVD on data matrix \\(\\mathbf{X}\\) while princomp uses eigenvalue decomposition on covariance or correlation matrix of the data matrix. We will use prcomp for our example."
  },
  {
    "objectID": "blog/posts/2017-03-18-interpreting-biplot/index.html#explore-data-using-biplot",
    "href": "blog/posts/2017-03-18-interpreting-biplot/index.html#explore-data-using-biplot",
    "title": "Interpretating Biplot",
    "section": "Explore data using biplot",
    "text": "Explore data using biplot\nNow lets compare the scores from the first two components, observations and variables in the data matrix. From the USArrests data, lets look at the top five states with highest and lowest arrest for each of these crimes.\n\nStates with highest arrestsStates with lowest arrests\n\n\n\nMurderAssaultUrbanPopRape\n\n\n\n\nCode\nUSArrests %>% arrange(-Murder) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State          Murder Assault UrbanPop  Rape\n  <chr>           <dbl>   <int>    <int> <dbl>\n1 Georgia          17.4     211       60  25.8\n2 Mississippi      16.1     259       44  17.1\n3 Florida          15.4     335       80  31.9\n4 Louisiana        15.4     249       66  22.2\n5 South Carolina   14.4     279       48  22.5\n\n\n\n\n\n\nCode\nUSArrests %>% arrange(-Assault) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State          Murder Assault UrbanPop  Rape\n  <chr>           <dbl>   <int>    <int> <dbl>\n1 North Carolina   13       337       45  16.1\n2 Florida          15.4     335       80  31.9\n3 Maryland         11.3     300       67  27.8\n4 Arizona           8.1     294       80  31  \n5 New Mexico       11.4     285       70  32.1\n\n\n\n\n\n\nCode\nUSArrests %>% arrange(-UrbanPop) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State         Murder Assault UrbanPop  Rape\n  <chr>          <dbl>   <int>    <int> <dbl>\n1 California       9       276       91  40.6\n2 New Jersey       7.4     159       89  18.8\n3 Rhode Island     3.4     174       87   8.3\n4 New York        11.1     254       86  26.1\n5 Massachusetts    4.4     149       85  16.3\n\n\n\n\n\n\nCode\nUSArrests %>% arrange(-Rape) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State      Murder Assault UrbanPop  Rape\n  <chr>       <dbl>   <int>    <int> <dbl>\n1 Nevada       12.2     252       81  46  \n2 Alaska       10       263       48  44.5\n3 California    9       276       91  40.6\n4 Colorado      7.9     204       78  38.7\n5 Michigan     12.1     255       74  35.1\n\n\n\n\n\n\n\n\nMurderAssaultUrbanPopRape\n\n\n\n\nCode\nUSArrests %>% arrange(Murder) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State         Murder Assault UrbanPop  Rape\n  <chr>          <dbl>   <int>    <int> <dbl>\n1 North Dakota     0.8      45       44   7.3\n2 Maine            2.1      83       51   7.8\n3 New Hampshire    2.1      57       56   9.5\n4 Iowa             2.2      56       57  11.3\n5 Vermont          2.2      48       32  11.2\n\n\n\n\n\n\nCode\nUSArrests %>% arrange(Assault) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State        Murder Assault UrbanPop  Rape\n  <chr>         <dbl>   <int>    <int> <dbl>\n1 North Dakota    0.8      45       44   7.3\n2 Hawaii          5.3      46       83  20.2\n3 Vermont         2.2      48       32  11.2\n4 Wisconsin       2.6      53       66  10.8\n5 Iowa            2.2      56       57  11.3\n\n\n\n\n\n\nCode\nUSArrests %>% arrange(UrbanPop) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State          Murder Assault UrbanPop  Rape\n  <chr>           <dbl>   <int>    <int> <dbl>\n1 Vermont           2.2      48       32  11.2\n2 West Virginia     5.7      81       39   9.3\n3 Mississippi      16.1     259       44  17.1\n4 North Dakota      0.8      45       44   7.3\n5 North Carolina   13       337       45  16.1\n\n\n\n\n\n\nCode\nUSArrests %>% arrange(Rape) %>% top_n()\n\n\n# A tidytable: 5 × 5\n  State         Murder Assault UrbanPop  Rape\n  <chr>          <dbl>   <int>    <int> <dbl>\n1 North Dakota     0.8      45       44   7.3\n2 Maine            2.1      83       51   7.8\n3 Rhode Island     3.4     174       87   8.3\n4 West Virginia    5.7      81       39   9.3\n5 New Hampshire    2.1      57       56   9.5\n\n\n\n\n\n\n\n\nComparing these highest and lowest arrests with the biplot, we can see a pattern. The weights corresponding to PC1 for all the variables are negative and are directed towards states like Florida, Nevada, and California. These states have the highest number of arrests for all of these crimes where as states that are in the oppositve direction like Iowa, North Dakota, and Vermont have the lowest arrest.\nSimilarly, UrbanPop have the highest weights corresponding to PC2 so the states in that direction such as California, Hawaii, and New Jersey have highest arrest related to UrbanPop. The states in the opposite direction, i.e. with negative PC2 scores such as Mississippi, North Carolina, Vermont, and South Carolina have the lowest arrest related to UrbanPop.\nThe weights for all variables are negative and towards states like . So in our data these states must have the highest arrests in all these crimes where as states like New Dakota, Vermont, and Iowa have the lowest arrests."
  },
  {
    "objectID": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html",
    "href": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html",
    "title": "How ANOVA analyze the variance",
    "section": "",
    "text": "Analysis of Variance (ANOVA) is a powerful statistical tool used to analyze the variance among group means and determine whether these differences are statistically significant. It’s commonly used in various fields such as agriculture, biology, psychology, and more to test hypotheses about different groups. Below are some practical examples:\nData for ANOVA could be collected through designed experiments or by sampling from populations. This article helps explain how ANOVA analyzes variance and identifies situations when these differences are significant, using both simulated and real data.\nOften we Analysis of Variance (ANOVA) to analyze the variances to find if different cases results in similar outcome and if the difference is significant. Following are some simple examples,\nConsider the following model with \\(i=3\\) groups and \\(j=n\\) observations,\n\\[\ny_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}, \\; i = 1, 2, 3 \\texttt{ and } j = 1, 2, \\ldots n\n\\]\nwhere, \\(\\tau_i\\) represetns the effect corresponding to group \\(i\\) and \\(\\varepsilon_{ij} \\sim \\mathrm{N}(0, \\sigma^2)\\), the usual assumption of linear model. In order to better understand how ANOVA finds the differences between groups and how the group mean and their standard deviation influence the results from ANOVA, we will explore the following four cases:"
  },
  {
    "objectID": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html#fitting-anova-model-for-each-cases",
    "href": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html#fitting-anova-model-for-each-cases",
    "title": "How ANOVA analyze the variance",
    "section": "Fitting ANOVA model for each cases",
    "text": "Fitting ANOVA model for each cases\n\n\nSimulate and fit ANOVA\ngenerate_data <- function(mean, sd, nobs = 50) {\n    Response <- rnorm(nobs, mean = mean, sd)\n    tidytable(ID = 1:nobs, Response = Response)\n}\n\nModel <- Design %>% \n    mutate(Data = map2(Mean, SD, generate_data)) %>% \n    unnest() %>%\n    nest(.by = \"Cases\") %>% \n    mutate(fit = map(data, function(dta) {\n        lm(Response ~ Group, data = dta)\n    }))\n\nModel\n\n\n# A tidytable: 4 × 3\n  Cases data                  fit   \n  <chr> <list>                <list>\n1 Case1 <tidytable [150 × 5]> <lm>  \n2 Case2 <tidytable [150 × 5]> <lm>  \n3 Case3 <tidytable [150 × 5]> <lm>  \n4 Case4 <tidytable [150 × 5]> <lm>"
  },
  {
    "objectID": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html#distribution-of-data",
    "href": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html#distribution-of-data",
    "title": "How ANOVA analyze the variance",
    "section": "Distribution of data",
    "text": "Distribution of data\n\n\nData distribution\nModel[, map_df(fit, broom::augment), by = Cases] %>%\n  ggplot(aes(Response, Group)) +\n  geom_boxplot(\n    aes(fill = Group, color = Group), \n    alpha = 0.25, width = 0.25\n  ) +\n  geom_point(aes(fill = Group),\n    position = position_jitter(height = 0.1),\n    shape = 21, size = 2, stroke = 0.25, alpha = 0.25\n  ) +\n  stat_summary(\n    fun = mean, geom = \"point\", aes(color = Group),\n    size = 2, shape = 21, fill = \"whitesmoke\", stroke = 0.75\n  ) +\n  facet_wrap(facets = vars(Cases), scales = \"free_x\") +\n  ggridges::geom_density_ridges(\n    aes(color = Group),\n    fill = NA,\n    panel_scaling = FALSE\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html#model-comparison",
    "href": "blog/posts/2021-03-29-how-anova-analyze-variance/index.html#model-comparison",
    "title": "How ANOVA analyze the variance",
    "section": "Model comparison",
    "text": "Model comparison\n\n\nANOVA for the four cases\nanova_result <- Model[, map_df(\n  .x = fit,\n  .f = ~ broom::tidy(anova(.x))\n), by = Cases] %>% rename(\n  DF = df,\n  SSE = sumsq,\n  MSE = meansq,\n  Statistic = statistic,\n  `p value` = p.value\n)\n\nanova_result %>% \n  mutate(\n    Cases = case_when(\n        Cases == \"Case1\" ~ \"Case 1: Similar group means with high variation within the groups\",\n        Cases == \"Case2\" ~ \"Case 2: Similar group means with low variation within the groups\",\n        Cases == \"Case3\" ~ \"Case 3: Distant group means with high variation within the groups\",\n        TRUE ~ \"Case 4: Distant group means with low variation within the groups\"\n    )\n  ) %>% \n  gt::gt(groupname_col = \"Cases\", rowname_col = \"term\") %>%\n  gt::fmt_number(columns = 4:6) %>%\n  gt::fmt_number(columns = 7, decimals = 4) %>% \n  gt::sub_missing(missing_text = \"\") %>% \n  gt::tab_options(\n    table.width = \"100%\",\n    row_group.font.weight = \"600\"\n  )\n\n\n\n\n\n\n  \n    \n      \n      DF\n      SSE\n      MSE\n      Statistic\n      p value\n    \n  \n  \n    \n      Case 1: Similar group means with high variation within the groups\n    \n    Group\n2\n32.09\n16.05\n0.64\n0.5304\n    Residuals\n147\n3,704.26\n25.20\n\n\n    \n      Case 2: Similar group means with low variation within the groups\n    \n    Group\n2\n1.98\n0.99\n1.00\n0.3714\n    Residuals\n147\n146.03\n0.99\n\n\n    \n      Case 3: Distant group means with high variation within the groups\n    \n    Group\n2\n1,951.86\n975.93\n50.15\n0.0000\n    Residuals\n147\n2,860.66\n19.46\n\n\n    \n      Case 4: Distant group means with low variation within the groups\n    \n    Group\n2\n2,516.51\n1,258.26\n1,263.82\n0.0000\n    Residuals\n147\n146.35\n1.00\n\n\n  \n  \n  \n\n\n\n\n\nInterpretetion\n\nCase 1Case 2Case 3Case 4\n\n\nThe results show a high p-value, indicating no significant difference between the groups due to high within-group variability.\n\n\nHere, the p-value is still high, suggesting no significant difference, but the small variance within groups provides clearer insights compared to Case 1.\n\n\nDespite the high variation within groups, the distant group means lead to a low p-value, indicating statistically significant differences among the groups.\n\n\nWith low within-group variation and distant means, the p-value remains extremely low, strongly indicating significant group differences.\n\n\n\nIn conclusion, ANOVA helps determine if there are significant differences between multiple group means by comparing variances within groups to variances between groups. The power of ANOVA lies in its ability to detect even subtle differences when variations are minimal within groups."
  },
  {
    "objectID": "blog/posts/2022-12-02-age-adjusted-rates/index.html",
    "href": "blog/posts/2022-12-02-age-adjusted-rates/index.html",
    "title": "Age Adjusted Rates in Epidemiology",
    "section": "",
    "text": "In general, rates means how fast something is changing usually over time. In epidemiology uses it to describe how quickly a disese occurs in a population. For example, 35 cases of melanoma cases in 100,000 person per year convey a the sense of speed of spread of disease in that population. Incidence rate and mortality rate are two examples that we will discuss further below.\nLet us use melanoma as a outcome in the following discussion. Here, we can calculate a crude incidence rate as,\n\\[\\mathcal{R} = \\frac{\\textsf{no. of melanoma cases}}{\\textsf{no. of person-year}} \\times \\textsf{some multiplier}\\]\nIn the case of mortality rate, we can replace the numerator of above expression by the number of melanoma deaths."
  },
  {
    "objectID": "blog/posts/2022-12-02-age-adjusted-rates/index.html#age-specific-rate",
    "href": "blog/posts/2022-12-02-age-adjusted-rates/index.html#age-specific-rate",
    "title": "Age Adjusted Rates in Epidemiology",
    "section": "Age-specific rate",
    "text": "Age-specific rate\nWeather to understand a broder prespecitve or to compare across population, these rates are often analyzed stratified by sex and age. This also helps to remove the confounding effection of these factors. The incidence/mortality rate per age-group is usually referred to as Age-specific rates where rates are computed for each age-groups. This is often desirable since factor age has a strong effect on mortality and incidence of most disease especially the cronic one."
  },
  {
    "objectID": "blog/posts/2022-12-02-age-adjusted-rates/index.html#age-adjusted-rate",
    "href": "blog/posts/2022-12-02-age-adjusted-rates/index.html#age-adjusted-rate",
    "title": "Age Adjusted Rates in Epidemiology",
    "section": "Age-adjusted rate",
    "text": "Age-adjusted rate\nMany research articles, however presents the age-adjusted rates. Age-adjusted rates are standardized (weighted) using some standard population age-structure. For example, many european studies on melanoma uses european standard age distribution. While any reasonal studies have also used world standard population. Cancer registry in their reports sometimes uses age-structure of that country in some given year. For instance, Norway2 and Finland3 have used the their population in 2014 as standard population in their recent cancer report while Australia have used 2001 Australian population4.\nStandardized (adjusted) rates makes comparison between the population possible. Figure Figure 1 shows the difference in the age distribution between world population and European population. Following table are some of the standard population often used in the study. Further on standard popuation see seer.cancer.gov5.\n\nStandard Population by Age\n\nTablePlot\n\n\n\n\nStandard Population\nus2000 <- tidytable(\n  `AgeGroup` = c(\n    \"0\", \"1-4\", \"5-9\", \"10-14\", \"15-19\", \n    \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\",\n    \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80-84\", \"85+\"\n  ), \n  US2000 = c(\n    13818, 55317, 72533, 73032, 72169, 66478, 64529, 71044, 80762, 81851, \n    72118, 62716, 48454, 38793, 34264, 31773, 26999, 17842, 15508\n  )\n)\nstd_pop <- as_tidytable(popEpi::stdpop18) %>% \n  mutate(agegroup = case_when(\n    agegroup == \"85\" ~ \"85+\", \n    TRUE ~ agegroup\n  )) %>% mutate(\n    age = case_when(\n      agegroup == \"0-4\" ~ list(c(\"0\", \"1-4\")),\n      TRUE ~ as.list(agegroup)\n    )\n  ) %>% unnest(age) %>% rename_with(\n    ~c(\"Age Group\", \"World\", \"Europe\", \"Nordic\", \"AgeGroup\")\n  ) %>% left_join(us2000) %>% \n  tidytable::mutate(across(everything(), function(x) {\n    if (x[1] == x[2]) x[2] <- NA\n    return(x)\n  })) %>% mutate(\n    AgeGroup = c(AgeGroup[1:2], rep(NA, length(AgeGroup) - 2))\n  )\n\ngt::gt(std_pop) %>% \n  gt::sub_missing(missing_text = \"\") %>% \n  gt::cols_label(AgeGroup = \"\") %>% \n  gt::opt_vertical_padding(0.5) %>%\n  gt::tab_style(\n    style = gt::cell_borders(sides = \"top\", weight = \"0\"),\n    locations = gt::cells_body(1:4, rows = 2)\n  ) %>% \n  gt::tab_options(column_labels.font.weight = \"bold\") %>% \n  gt::tab_header(\"Standard Population by Age Group\")\n\n\n\n\n\n\n  \n    \n      Standard Population by Age Group\n    \n    \n    \n      Age Group\n      World\n      Europe\n      Nordic\n      \n      US2000\n    \n  \n  \n    0-4\n12000\n8000\n5900\n0\n13818\n    \n\n\n\n1-4\n55317\n    5-9\n10000\n7000\n6600\n\n72533\n    10-14\n9000\n7000\n6200\n\n73032\n    15-19\n9000\n7000\n5800\n\n72169\n    20-24\n8000\n7000\n6100\n\n66478\n    25-29\n8000\n7000\n6800\n\n64529\n    30-34\n6000\n7000\n7300\n\n71044\n    35-39\n6000\n7000\n7300\n\n80762\n    40-44\n6000\n7000\n7000\n\n81851\n    45-49\n6000\n7000\n6900\n\n72118\n    50-54\n5000\n7000\n7400\n\n62716\n    55-59\n4000\n6000\n6100\n\n48454\n    60-64\n4000\n5000\n4800\n\n38793\n    65-69\n3000\n4000\n4100\n\n34264\n    70-74\n2000\n3000\n3900\n\n31773\n    75-79\n1000\n2000\n3500\n\n26999\n    80-84\n500\n1000\n2400\n\n17842\n    85+\n500\n1000\n1900\n\n15508\n  \n  \n  \n\n\n\n\n\n\n\n\nStandard population plot\npop_data <- as_tidytable(popEpi::stdpop18) %>% \n  mutate(nordic = NULL, id = 1:n())\n\npop_data %>% \n  mutate(world = world * -1) %>% \n  pivot_longer(cols = c(world, europe)) %>% \n  arrange(id) %>% \n  ggplot(aes(value, reorder(agegroup, id), fill = name)) +\n  geom_col(width = 1, color = \"#f0f0f0\", size = 0.2) +\n  theme_minimal() +\n  ggthemes::scale_fill_economist(labels = stringr::str_to_title) +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_line(color = \"red\")\n  ) +\n  scale_x_continuous(labels = abs, expand = expansion()) +\n  scale_y_discrete(expand = expansion()) +\n  labs(\n    x = \"Number of person\",\n    y = NULL,\n    fill = \"Population\"\n  ) +\n  annotate(\n    x = -Inf, y = Inf,\n    geom = \"text\", label = \"World\",\n    hjust = -0.5, vjust = 1.5\n  ) +\n  annotate(\n    x = Inf, y = Inf,\n    geom = \"text\", label = \"Europe\",\n    hjust = 1.5, vjust = 1.5\n  )\n\n\n\n\n\nFigure 1: World and European Standard Population"
  },
  {
    "objectID": "blog/posts/2022-12-02-age-adjusted-rates/index.html#calculating-age-standardized-rate",
    "href": "blog/posts/2022-12-02-age-adjusted-rates/index.html#calculating-age-standardized-rate",
    "title": "Age Adjusted Rates in Epidemiology",
    "section": "Calculating age-standardized rate",
    "text": "Calculating age-standardized rate\nThe age-standardized rate (ASR) is calculated as,\n\\[\\text{Age.Std. Rate} = \\frac{\\sum_i\\mathcal{R}_i\\mathcal{w}_i}{\\sum_i\\mathcal{w}_i}\\]\nwhere, \\(\\mathcal{w}_i\\) is the weight corresponding to \\(i^\\text{th}\\) age-group in the reference population.\nLet’s explore further with an example from melanoma cases from Australia."
  },
  {
    "objectID": "blog/posts/2022-12-02-age-adjusted-rates/index.html#example",
    "href": "blog/posts/2022-12-02-age-adjusted-rates/index.html#example",
    "title": "Age Adjusted Rates in Epidemiology",
    "section": "Example",
    "text": "Example\nThe following example have used the Austrailian cancer data with 5-year age-group6 after filtering melanoma cases from 1982 to 2018. The dataset has yearly count and age-specific incidence rate of melanoma for men and women.\nLet us use the above European standard population to find the yearly age-standardized incidence by sex.\n\n\nAge-specific Data\nstd_pop <- as_tidytable(popEpi::stdpop18) %>% \n  rename_with(~c(\"AgeGroup\", \"World\", \"Europe\", \"Nordic\")) %>% \n  mutate(AgeGroup = case_when(\n    AgeGroup == \"85\" ~ \"85+\", \n    TRUE ~ AgeGroup\n  )) %>% mutate(\n    across(World:Nordic, prop.table)\n  )\n\ndata <- tidytable::fread(\"melanoma.csv\") %>% \n  mutate(AgeGroup = case_when(\n    AgeGroup %in% c(\"85-89\", \"90+\") ~ \"85+\",\n    TRUE ~ AgeGroup\n  ))\n\nasp_data <- data %>% \n  left_join(\n    std_pop %>% select(AgeGroup, World), \n    by = \"AgeGroup\"\n  )\n\nhead(asp_data)\n\n\n# A tidytable: 6 × 6\n   Year Sex   AgeGroup Count   ASR World\n  <int> <chr> <chr>    <int> <dbl> <dbl>\n1  1982 Males 0-4          0   0    0.12\n2  1982 Males 5-9          0   0    0.1 \n3  1982 Males 10-14        6   0.9  0.09\n4  1982 Males 15-19       30   4.6  0.09\n5  1982 Males 20-24       52   7.7  0.08\n6  1982 Males 25-29       83  13.1  0.08\n\n\n\n\nAge-standardized Rate\nasp <- asp_data %>% \n  group_by(Year, Sex) %>% \n  summarize(AgeAdjRate = sum(ASR * World))\n\nasp %>% \n  group_by(Sex) %>% \n  slice_tail(8) %>% \n  ungroup() %>% \n  tidytable::pivot_wider(\n    names_from = \"Year\",\n    values_from = \"AgeAdjRate\"\n  ) %>% \n  gt::gt() %>% \n  gt::opt_vertical_padding(0.5) %>% \n  gt::fmt_number(columns = -1) %>% \n  gt::tab_options(table.width = \"100%\")\n\n\n\n\n\n\n  \n    \n      Sex\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n    \n  \n  \n    Females\n28.91\n29.69\n30.30\n30.52\n30.86\n32.16\n31.51\n32.40\n    Males\n41.43\n42.38\n43.13\n42.84\n43.73\n45.31\n45.27\n45.04\n  \n  \n  \n\n\n\n\nNow, let us compare the age-specific rates (crude rates) and age-standardized rates with a plot,\n\nCompare the age-specific rates and age-adjusted rates\n\n\nAge-specific rates vs Age-adjusted rates\nggplot() +\n  geom_line(\n    data = data,\n    aes(\n      x = Year,\n      y = ASR,\n      group = AgeGroup,\n      color = \"Crude\"\n    )\n  ) +\n  geom_line(\n    data = asp,\n    aes(\n      x = Year,\n      y = AgeAdjRate,\n      group = 1,\n      color = \"Age-adjusted\"\n    )\n  ) +\n  geom_text(\n    data = data[Year == max(Year)],\n    check_overlap = TRUE,\n    size = rel(2),\n    color = \"#0f0f0f\",\n    aes(\n      x = Year + 2,\n      y = ASR,\n      label = AgeGroup\n    )\n  ) +\n  scale_x_continuous(breaks = scales::breaks_extended(8)) +\n  scale_y_continuous(breaks = scales::breaks_extended(8)) +\n  scale_color_manual(NULL, values = c(\"firebrick\", \"grey\")) +\n  facet_grid(cols = vars(Sex)) +\n  theme_minimal() +\n  theme(\n    panel.border = element_rect(fill = NA, color = \"darkgrey\"),\n    legend.position = \"inside\",\n    legend.position.inside = c(0, 1),\n    legend.justification = c(0, 1)\n  ) +\n  labs(\n    x = \"Diagnosis year\",\n    y = paste(\n      \"Age-adjusted incidence rate\",\n      \"per 100,000 person year\",\n      sep = \"\\n\"\n    )\n  )\n\n\n\n\n\nFigure 2: Age-specific and Age-adjusted rate showing why age-adjustement is necessary\n\n\n\n\n\n\nCompare the age-adjusted rates by sex\n\n\nAge-adjusted rates by sex\nggplot(asp, aes(x = Year, y = AgeAdjRate, color = Sex)) +\n  geom_line() +\n  geom_point(fill = \"whitesmoke\", shape = 21) +\n  scale_x_continuous(breaks = scales::breaks_extended(8)) +\n  scale_y_continuous(breaks = scales::breaks_extended(8)) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_minimal() +\n  theme(\n    panel.border = element_rect(fill = NA, color = \"darkgrey\"),\n    legend.position = c(0, 1),\n    legend.justification = c(0, 1)\n  ) +\n  labs(\n    x = \"Diagnosis year\",\n    y = paste(\n      \"Age-adjusted incidence rate\",\n      \"per 100,000 person year\",\n      sep = \"\\n\"\n    )\n  ) +\n  expand_limits(y = 0)\n\n\n\n\n\nFigure 3: Age-adjusted rates by sex for melanoma patients"
  },
  {
    "objectID": "blog/posts/2022-12-02-age-adjusted-rates/index.html#discussion",
    "href": "blog/posts/2022-12-02-age-adjusted-rates/index.html#discussion",
    "title": "Age Adjusted Rates in Epidemiology",
    "section": "Discussion",
    "text": "Discussion\nFigure Figure 2 shows that the incidence of melanoma has larger difference in men between the age-groups than in women and men also have a sharp increase in older age group. In addition, the Figure Figure 3 shows that males have higher age-adjusted incidence of melanoma than women in Australia and this trend is increasing over time with rapid increase before 1983 before a drop.\nAge-adjusted rates are useful for comparing rates between population but it cannot give the interpretation required for comparing within a population or over a time period in that population. This is one of the reason, cancer registry uses the internal (population structure of their own population) to compute the age-adjusted rates."
  }
]